{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNteI6dmesVZjR268SH9+jk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doloresgarro/Deep-Learning/blob/main/Pr%C3%A1ctica4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Práctica 4 - Deep Learning**\n",
        "*Multiperceptrón*"
      ],
      "metadata": {
        "id": "a6oJMbf-Y4AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 1**"
      ],
      "metadata": {
        "id": "_N2Ryy2jYz4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*a) En base a esta información, indique:*\n",
        "\n",
        "*▪ Cuántos ejemplos se utilizaron en el entrenamiento.*\n",
        "\n",
        "\n",
        "*▪ Cuántas clases puede reconocer este multiperceptrón.*\n",
        "\n",
        "*▪ Cuál es la precisión (accuracy) de la red sobre el conjunto de ejemplos completo.*\n",
        "\n",
        "*▪ Cuáles son los valores de precisión de la red al responder por cada uno de los valores de clase (precision).*\n",
        "\n",
        "*▪ Cuáles son los valores de sensibilidad de la red al responder por cada uno de los valores de clase (recall).*"
      ],
      "metadata": {
        "id": "dfxR5nDUZgIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Se entreno una red neuronal multiperceptrón para resolver un problema de clasificación y al medir su desempeño sobre el conjunto de datos de entrenamiento se obtuvo la siguiente matriz de confusión:*"
      ],
      "metadata": {
        "id": "0q_lh8A2aIDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWS_CfR4Opou"
      },
      "outputs": [],
      "source": [
        "import pandas as pd      # para trabajar con archivos de datos csv, excel, etc: https://pandas.pydata.org/docs/getting_started/tutorials.html\n",
        "import numpy as np\n",
        "\n",
        "# calcula las metricas precision, recall, f1-score y accuracy a partir de la matriz de confusion\n",
        "# retorna tupla: ( precision, recall, f1_score, accuracy )\n",
        "def calcular_metricas(conf_mat):\n",
        "    precision = np.zeros(conf_mat.shape[0])  # inicializa en 0 el vector precisión, conf_mat.shape[0] --> indica el num de filas de la matriz con_mat\n",
        "                                             # por lo tanto precisión va a ser una vector inicializado en 0 con el num de filas de la matriz recibida como parámetro\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        precision[i] = conf_mat[i][i]/sum(conf_mat.T[i])  # en c/ pos divide el valor de la matriz en esa f y c por la suma de todos los elem de la matriz en la columna i\n",
        "\n",
        "    recall = np.zeros(conf_mat.shape[0])   # recall lo mismo q lo anterior\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        recall[i] = conf_mat[i][i]/sum(conf_mat[i])\n",
        "\n",
        "    f1_score = 2* (precision*recall) /(precision+recall)\n",
        "\n",
        "    accuracy =  0\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        accuracy+=conf_mat[i][i]\n",
        "    accuracy/= conf_mat.sum()        # conf_mat.sum() --> suma de elems de la matriz conf_mat\n",
        "\n",
        "    return ( precision, recall, f1_score, accuracy )\n",
        "\n",
        "# el parámetro metricas es una tupla ( precision, recall, f1_score, accuracy )\n",
        "def imprimir_metricas( metricas ):\n",
        "    (precision, recall, f1_score, accuracy) = metricas\n",
        "    print('\\n clase   precision    recall    f1-score')\n",
        "    for i in range(0, len(precision)):\n",
        "        print('%5d %10.2f %10.2f %10.2f' % (i, precision[i], recall[i], f1_score[i]))\n",
        "    print('\\naccuracy: %6.2f\\n' % accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion = np.array([\n",
        "           [ 17, 0, 1, 0, 1 ],\n",
        "           [ 0, 12, 0, 0, 0 ],\n",
        "           [ 0, 0, 12, 0, 0 ],\n",
        "           [ 2, 0, 0, 38, 0 ],\n",
        "           [ 0, 8, 0, 0, 61 ]\n",
        "           ])\n",
        "\n",
        "print('\\n Matriz de Confusión:')\n",
        "print(confusion, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "#calcula métricas de forma manual\n",
        "metricas = calcular_metricas(confusion)\n",
        "imprimir_metricas(metricas)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfnoh6PSZRDg",
        "outputId": "f0831a67-dde1-4796-a6d1-b5432be785a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Matriz de Confusión:\n",
            "[[17  0  1  0  1]\n",
            " [ 0 12  0  0  0]\n",
            " [ 0  0 12  0  0]\n",
            " [ 2  0  0 38  0]\n",
            " [ 0  8  0  0 61]] \n",
            "\n",
            "\n",
            " clase   precision    recall    f1-score\n",
            "    0       0.89       0.89       0.89\n",
            "    1       0.60       1.00       0.75\n",
            "    2       0.92       1.00       0.96\n",
            "    3       1.00       0.95       0.97\n",
            "    4       0.98       0.88       0.93\n",
            "\n",
            "accuracy:   0.92\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisión --> proporción de predicciones correctas sobre una clase\n",
        "\n",
        "Recall --> proporción de ejemplos de una clase que correctamente clasificados\n",
        "\n",
        "Acurrancy --> cantidad de aciertos sobre el total de ejemplos\n"
      ],
      "metadata": {
        "id": "IuSBcQ97gxPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) La clase con mejor valor de F1-score es la clase 3"
      ],
      "metadata": {
        "id": "FvALAnd8hnYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 2**"
      ],
      "metadata": {
        "id": "_qz0tBvHo_89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Se desea utilizar una red multiperceptrón para reconocer muestras de tres variedades diferentes de trigo:*\n",
        "\n",
        "*Kama, Rosa y Canadiense.*\n",
        "\n",
        "*Para entrenarla se utilizará una parte de los ejemplos del archivo\n",
        "SEMILLAS.CSV. Estos datos fueron utilizados en el ejercicio 3 la práctica 2.*\n",
        "\n",
        "\n",
        "\n",
        "*a) Con respecto a la arquitectura, indique:*\n",
        "\n",
        "*▪ La cantidad de neuronas de la capa de entrada.*\n",
        "\n",
        "\n",
        "*▪ La cantidad de neuronas de la capa de salida.*\n",
        "\n",
        "*▪ La cantidad de pesos (arcos) que tiene la red si se utiliza una única capa oculta formada por 4 neuronas*"
      ],
      "metadata": {
        "id": "lTvT60PMpEqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)\n",
        "- entrada: 7 + bias = 8\n",
        "- salida: 3 porque tengo tres variedades de semillas\n",
        "- capa oculta: 4 neuronas\n",
        "- arcos:\n",
        "\n",
        "n⋅h1​+∑​hi​⋅hi+1​+hN​⋅m\n",
        "\n",
        "\n",
        "\n",
        "*   n --> neuronas en capa de entrada\n",
        "*   m --> neuronas en capa de salida\n",
        "*   hi --> neuronas en capa oculta\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pTSocv21xvdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import grafica as gr\n",
        "from sklearn import preprocessing, metrics, model_selection\n",
        "\n",
        "df = pd.read_csv('Semillas.csv')\n",
        "\n",
        "X = np.array(df.iloc[:, 0:4])\n",
        "nEj = X.shape[0] # cantidad de ejemplos (muestras) en los datos\n",
        "\n",
        "#column_names = df.columns\n",
        "#print(column_names)\n",
        "\n",
        "nomClases = pd.unique(df['Clase'])\n",
        "#-- la red tendrá una salida para cada tipo de flor\n",
        "salidas = len(nomClases)\n",
        "\n",
        "#-- la salida debe ser numérica --\n",
        "# transforma las clases en df['Clase'] en valores numéricos basados\n",
        "# en su posición en el arreglo nomClases.\n",
        "clase = df['Clase']\n",
        "Y=np.zeros(nEj)\n",
        "for s in range(nEj):\n",
        "    Y[s]=np.argwhere(nomClases == clase[s])\n",
        "Y = Y.astype(int)\n",
        "\n",
        "\n",
        "#--- CONJUNTOS DE ENTRENAMIENTO Y TESTEO ---\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split( \\\n",
        "        X, Y, test_size=0.30) #, random_state=42)\n",
        "\n",
        "Y_trainB = np.zeros((len(Y_train), salidas))\n",
        "for o in range(len(Y_train)):\n",
        "    Y_trainB[o, Y_train[o]]=1\n",
        "\n",
        "normalizarEntrada = 1  # 1 si normaliza; 0 si no\n",
        "\n",
        "if normalizarEntrada:\n",
        "    # Escala los valores entre 0 y 1\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    # se aplica la normalización a los conjuntos de datos X_train y X_test\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "entradas = X_train.shape[1] # obtiene las entradas de la red\n",
        "ocultas = 4 # neuronas en la capa oculta\n",
        "salidas = Y_trainB.shape[1] # cantidad de clases a predecir (3)\n",
        "print(entradas)\n",
        "print(ocultas)\n",
        "print(salidas)\n",
        "\n",
        "# inicializo pesos y bias\n",
        "# se inicializa W1 como matriz de pesos de la capa oculta\n",
        "# se inicializa W2 como matriz de pesos de la capa de salida\n",
        "# pesos y bias --> se inicializan aleatoriamente entre -0.5 y 0.5\n",
        "# los w se van a ir ajustando durante el entrenamiento\n",
        "W1 = np.random.uniform(-0.5,0.5,[ocultas, entradas])\n",
        "b1 = np.random.uniform(-0.5,0.5, [ocultas,1]) # 1 --> indica una única neurona\n",
        "W2 = np.random.uniform(-0.5,0.5,[salidas, ocultas])\n",
        "b2 = np.random.uniform(-0.5,0.5, [salidas,1])\n",
        "\n",
        "#=====  Calcular el error actual =====\n",
        "FunH = 'logsig'\n",
        "FunO = 'tansig'\n",
        "if (FunO == 'tansig'):\n",
        "    Y_trainB = 2 * Y_trainB - 1\n",
        "# capa oculta\n",
        "#FunH = 'tanh' # mapea entradas al rango [-1,1]\n",
        "# capa de salida\n",
        "#FunO = 'sigmoid' # mapea entradas al rango [0,1]\n",
        "#if (FunO == 'sigmoid'):\n",
        "#    Y_trainB = 2 * Y_trainB - 1 # --> ajusta etiquetas de clase al rango [-1,1]\n",
        "\n",
        "# --- Calcular la rta.de la red para TODOS los ejemplos ---\n",
        "# w1 --> matriz de pesos que conecta las neuronas de capa oculta con las caract de entrada\n",
        "# X_train --> matriz de caract de entrada. Num de clases de entrada\n",
        "# b1 --> biass\n",
        "NetasH = W1 @ X_train.T + b1\n",
        "\n",
        "# cantEj = P.shape[0]\n",
        "# netasH = np.zeros([ocultas, cantEj])\n",
        "# for i in range(cantEj):\n",
        "#     for o in range(ocultas):\n",
        "#         netasH[o,i]=b1[o]\n",
        "#         for e in range(entradas):\n",
        "#             netasH[o,i] = netasH[o,i] + W1[o,e]*P[i,e]\n",
        "\n",
        "# cálculo de salidas de la red para cada capa\n",
        "SalidasH = gr.evaluar(FunH, NetasH) # salidas de capa oculta\n",
        "NetasO = W2 @ SalidasH + b2 # entradas de la capa de salidda\n",
        "SalidasO = gr.evaluar(FunO, NetasO) # salidas de la capa de salida\n",
        "\n",
        "# -- calcular el error --\n",
        "AVGError = np.mean((Y_trainB.T - SalidasO)**2)\n",
        "\n",
        "alfa = 0.1\n",
        "CotaError = 1.0e-15\n",
        "MAX_ITERA = 800\n",
        "ite = 0\n",
        "errorAnt = 1\n",
        "while ( abs(AVGError-errorAnt) > CotaError ) and ( ite < MAX_ITERA ):\n",
        "    errorAnt = AVGError\n",
        "    for p in range(len(X_train)):   #para cada ejemplo\n",
        "        # propagar el ejemplo hacia adelante\n",
        "        netasH = W1 @ X_train[p:p+1,:].T + b1\n",
        "        salidasH = gr.evaluar(FunH, netasH)\n",
        "        netasO = W2 @ salidasH + b2\n",
        "        salidasO = gr.evaluar(FunO, netasO)\n",
        "\n",
        "        # calcular los errores en ambas capas\n",
        "        ErrorSalida = Y_trainB[p:p+1,:].T-salidasO\n",
        "        deltaO = ErrorSalida * gr.evaluarDerivada(FunO,salidasO)\n",
        "        deltaH = gr.evaluarDerivada(FunH,salidasH)*(W2.T @ deltaO)\n",
        "\n",
        "        # corregir todos los pesos\n",
        "        W1 = W1 + alfa * deltaH @ X_train[p:p+1,:]\n",
        "        b1 = b1 + alfa * deltaH\n",
        "        W2 = W2 + alfa * deltaO @ salidasH.T\n",
        "        b2 = b2 + alfa * deltaO\n",
        "\n",
        "    # Recalcular AVGError\n",
        "    NetasH = W1 @ X_train.T + b1\n",
        "    SalidasH = gr.evaluar(FunH, NetasH)\n",
        "    NetasO = W2 @ SalidasH + b2\n",
        "    SalidasO = gr.evaluar(FunO, NetasO)\n",
        "    AVGError = np.mean((Y_trainB.T - SalidasO)**2)\n",
        "\n",
        "    ite = ite + 1\n",
        "    print(\"ite = %3d   error = %.8f\" % (ite, abs(AVGError-errorAnt)))\n",
        "\n",
        "if (FunO == 'tansig'):\n",
        "    y_pred = 2*((SalidasO>0) * 1)-1\n",
        "\n",
        "if (FunO == 'logsig'):\n",
        "    y_pred = (SalidasO>0.5) * 1\n",
        "\n",
        "Y_pred = np.argmax(y_pred,axis=0)\n",
        "#metrics.accuracy_score(y_test,evaluar(FUN,pred_test))\n",
        "\n",
        "print(\"%% aciertos X_train : %.3f\" % metrics.accuracy_score(Y_train,Y_pred))\n",
        "\n",
        "report = metrics.classification_report(Y_train,Y_pred)\n",
        "print(\"Confusion matrix:\\n%s\" % report)\n",
        "MM = metrics.confusion_matrix(Y_train,Y_pred)\n",
        "print(\"Confusion matrix:\\n%s\" % MM)\n",
        "\n",
        "#--- aplicando la red a los datos de testeo ---\n",
        "NetasH = W1 @ X_test.T + b1\n",
        "SalidasH = gr.evaluar(FunH, NetasH)\n",
        "NetasO = W2 @ SalidasH + b2\n",
        "SalidasO = gr.evaluar(FunO, NetasO)\n",
        "\n",
        "if (FunO == 'tansig'):\n",
        "    y_predTest = 2*((SalidasO>0) * 1)-1\n",
        "\n",
        "if (FunO == 'logsig'):\n",
        "    y_predTest = (SalidasO>0.5) * 1\n",
        "\n",
        "Y_predTest = np.argmax(y_predTest,axis=0)\n",
        "\n",
        "print(\"%% aciertos X_test : %.3f\" % metrics.accuracy_score(Y_test,Y_predTest))\n",
        "\n",
        "report = metrics.classification_report(Y_test,Y_predTest)\n",
        "print(\"Confusion matrix:\\n%s\" % report)\n",
        "MM = metrics.confusion_matrix(Y_test,Y_predTest)\n",
        "print(\"Confusion matrix:\\n%s\" % MM)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZUL9R78Est3",
        "outputId": "521d4c54-ffd4-41c6-d720-91654cdaaba3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "4\n",
            "3\n",
            "ite =   1   error = 0.42461782\n",
            "ite =   2   error = 0.13363024\n",
            "ite =   3   error = 0.16860556\n",
            "ite =   4   error = 0.07329339\n",
            "ite =   5   error = 0.03859649\n",
            "ite =   6   error = 0.03153529\n",
            "ite =   7   error = 0.03124275\n",
            "ite =   8   error = 0.02982490\n",
            "ite =   9   error = 0.02556523\n",
            "ite =  10   error = 0.02019221\n",
            "ite =  11   error = 0.01533402\n",
            "ite =  12   error = 0.01152827\n",
            "ite =  13   error = 0.00872892\n",
            "ite =  14   error = 0.00672816\n",
            "ite =  15   error = 0.00531679\n",
            "ite =  16   error = 0.00432706\n",
            "ite =  17   error = 0.00363431\n",
            "ite =  18   error = 0.00314778\n",
            "ite =  19   error = 0.00280135\n",
            "ite =  20   error = 0.00254663\n",
            "ite =  21   error = 0.00234850\n",
            "ite =  22   error = 0.00218233\n",
            "ite =  23   error = 0.00203207\n",
            "ite =  24   error = 0.00188849\n",
            "ite =  25   error = 0.00174752\n",
            "ite =  26   error = 0.00160846\n",
            "ite =  27   error = 0.00147249\n",
            "ite =  28   error = 0.00134149\n",
            "ite =  29   error = 0.00121733\n",
            "ite =  30   error = 0.00110142\n",
            "ite =  31   error = 0.00099467\n",
            "ite =  32   error = 0.00089741\n",
            "ite =  33   error = 0.00080957\n",
            "ite =  34   error = 0.00073077\n",
            "ite =  35   error = 0.00066043\n",
            "ite =  36   error = 0.00059787\n",
            "ite =  37   error = 0.00054235\n",
            "ite =  38   error = 0.00049315\n",
            "ite =  39   error = 0.00044959\n",
            "ite =  40   error = 0.00041102\n",
            "ite =  41   error = 0.00037685\n",
            "ite =  42   error = 0.00034656\n",
            "ite =  43   error = 0.00031966\n",
            "ite =  44   error = 0.00029576\n",
            "ite =  45   error = 0.00027446\n",
            "ite =  46   error = 0.00025546\n",
            "ite =  47   error = 0.00023846\n",
            "ite =  48   error = 0.00022323\n",
            "ite =  49   error = 0.00020953\n",
            "ite =  50   error = 0.00019719\n",
            "ite =  51   error = 0.00018603\n",
            "ite =  52   error = 0.00017592\n",
            "ite =  53   error = 0.00016673\n",
            "ite =  54   error = 0.00015834\n",
            "ite =  55   error = 0.00015068\n",
            "ite =  56   error = 0.00014365\n",
            "ite =  57   error = 0.00013719\n",
            "ite =  58   error = 0.00013123\n",
            "ite =  59   error = 0.00012572\n",
            "ite =  60   error = 0.00012061\n",
            "ite =  61   error = 0.00011587\n",
            "ite =  62   error = 0.00011145\n",
            "ite =  63   error = 0.00010733\n",
            "ite =  64   error = 0.00010348\n",
            "ite =  65   error = 0.00009987\n",
            "ite =  66   error = 0.00009649\n",
            "ite =  67   error = 0.00009331\n",
            "ite =  68   error = 0.00009032\n",
            "ite =  69   error = 0.00008750\n",
            "ite =  70   error = 0.00008485\n",
            "ite =  71   error = 0.00008234\n",
            "ite =  72   error = 0.00007996\n",
            "ite =  73   error = 0.00007771\n",
            "ite =  74   error = 0.00007557\n",
            "ite =  75   error = 0.00007355\n",
            "ite =  76   error = 0.00007162\n",
            "ite =  77   error = 0.00006978\n",
            "ite =  78   error = 0.00006802\n",
            "ite =  79   error = 0.00006635\n",
            "ite =  80   error = 0.00006475\n",
            "ite =  81   error = 0.00006322\n",
            "ite =  82   error = 0.00006175\n",
            "ite =  83   error = 0.00006033\n",
            "ite =  84   error = 0.00005898\n",
            "ite =  85   error = 0.00005768\n",
            "ite =  86   error = 0.00005642\n",
            "ite =  87   error = 0.00005521\n",
            "ite =  88   error = 0.00005404\n",
            "ite =  89   error = 0.00005291\n",
            "ite =  90   error = 0.00005182\n",
            "ite =  91   error = 0.00005077\n",
            "ite =  92   error = 0.00004975\n",
            "ite =  93   error = 0.00004876\n",
            "ite =  94   error = 0.00004781\n",
            "ite =  95   error = 0.00004688\n",
            "ite =  96   error = 0.00004598\n",
            "ite =  97   error = 0.00004510\n",
            "ite =  98   error = 0.00004425\n",
            "ite =  99   error = 0.00004342\n",
            "ite = 100   error = 0.00004262\n",
            "ite = 101   error = 0.00004184\n",
            "ite = 102   error = 0.00004108\n",
            "ite = 103   error = 0.00004034\n",
            "ite = 104   error = 0.00003962\n",
            "ite = 105   error = 0.00003892\n",
            "ite = 106   error = 0.00003823\n",
            "ite = 107   error = 0.00003756\n",
            "ite = 108   error = 0.00003691\n",
            "ite = 109   error = 0.00003628\n",
            "ite = 110   error = 0.00003566\n",
            "ite = 111   error = 0.00003505\n",
            "ite = 112   error = 0.00003446\n",
            "ite = 113   error = 0.00003389\n",
            "ite = 114   error = 0.00003332\n",
            "ite = 115   error = 0.00003277\n",
            "ite = 116   error = 0.00003223\n",
            "ite = 117   error = 0.00003171\n",
            "ite = 118   error = 0.00003119\n",
            "ite = 119   error = 0.00003069\n",
            "ite = 120   error = 0.00003020\n",
            "ite = 121   error = 0.00002972\n",
            "ite = 122   error = 0.00002925\n",
            "ite = 123   error = 0.00002879\n",
            "ite = 124   error = 0.00002833\n",
            "ite = 125   error = 0.00002789\n",
            "ite = 126   error = 0.00002746\n",
            "ite = 127   error = 0.00002704\n",
            "ite = 128   error = 0.00002662\n",
            "ite = 129   error = 0.00002622\n",
            "ite = 130   error = 0.00002582\n",
            "ite = 131   error = 0.00002543\n",
            "ite = 132   error = 0.00002505\n",
            "ite = 133   error = 0.00002468\n",
            "ite = 134   error = 0.00002431\n",
            "ite = 135   error = 0.00002395\n",
            "ite = 136   error = 0.00002360\n",
            "ite = 137   error = 0.00002326\n",
            "ite = 138   error = 0.00002292\n",
            "ite = 139   error = 0.00002259\n",
            "ite = 140   error = 0.00002226\n",
            "ite = 141   error = 0.00002194\n",
            "ite = 142   error = 0.00002163\n",
            "ite = 143   error = 0.00002133\n",
            "ite = 144   error = 0.00002103\n",
            "ite = 145   error = 0.00002073\n",
            "ite = 146   error = 0.00002044\n",
            "ite = 147   error = 0.00002016\n",
            "ite = 148   error = 0.00001988\n",
            "ite = 149   error = 0.00001961\n",
            "ite = 150   error = 0.00001934\n",
            "ite = 151   error = 0.00001908\n",
            "ite = 152   error = 0.00001883\n",
            "ite = 153   error = 0.00001857\n",
            "ite = 154   error = 0.00001833\n",
            "ite = 155   error = 0.00001808\n",
            "ite = 156   error = 0.00001785\n",
            "ite = 157   error = 0.00001761\n",
            "ite = 158   error = 0.00001739\n",
            "ite = 159   error = 0.00001716\n",
            "ite = 160   error = 0.00001694\n",
            "ite = 161   error = 0.00001673\n",
            "ite = 162   error = 0.00001652\n",
            "ite = 163   error = 0.00001631\n",
            "ite = 164   error = 0.00001611\n",
            "ite = 165   error = 0.00001591\n",
            "ite = 166   error = 0.00001571\n",
            "ite = 167   error = 0.00001552\n",
            "ite = 168   error = 0.00001533\n",
            "ite = 169   error = 0.00001515\n",
            "ite = 170   error = 0.00001497\n",
            "ite = 171   error = 0.00001479\n",
            "ite = 172   error = 0.00001462\n",
            "ite = 173   error = 0.00001445\n",
            "ite = 174   error = 0.00001428\n",
            "ite = 175   error = 0.00001412\n",
            "ite = 176   error = 0.00001396\n",
            "ite = 177   error = 0.00001380\n",
            "ite = 178   error = 0.00001365\n",
            "ite = 179   error = 0.00001349\n",
            "ite = 180   error = 0.00001335\n",
            "ite = 181   error = 0.00001320\n",
            "ite = 182   error = 0.00001306\n",
            "ite = 183   error = 0.00001292\n",
            "ite = 184   error = 0.00001279\n",
            "ite = 185   error = 0.00001265\n",
            "ite = 186   error = 0.00001252\n",
            "ite = 187   error = 0.00001239\n",
            "ite = 188   error = 0.00001227\n",
            "ite = 189   error = 0.00001215\n",
            "ite = 190   error = 0.00001203\n",
            "ite = 191   error = 0.00001191\n",
            "ite = 192   error = 0.00001179\n",
            "ite = 193   error = 0.00001168\n",
            "ite = 194   error = 0.00001157\n",
            "ite = 195   error = 0.00001147\n",
            "ite = 196   error = 0.00001136\n",
            "ite = 197   error = 0.00001126\n",
            "ite = 198   error = 0.00001116\n",
            "ite = 199   error = 0.00001106\n",
            "ite = 200   error = 0.00001096\n",
            "ite = 201   error = 0.00001087\n",
            "ite = 202   error = 0.00001078\n",
            "ite = 203   error = 0.00001069\n",
            "ite = 204   error = 0.00001060\n",
            "ite = 205   error = 0.00001051\n",
            "ite = 206   error = 0.00001043\n",
            "ite = 207   error = 0.00001035\n",
            "ite = 208   error = 0.00001027\n",
            "ite = 209   error = 0.00001019\n",
            "ite = 210   error = 0.00001012\n",
            "ite = 211   error = 0.00001004\n",
            "ite = 212   error = 0.00000997\n",
            "ite = 213   error = 0.00000990\n",
            "ite = 214   error = 0.00000983\n",
            "ite = 215   error = 0.00000976\n",
            "ite = 216   error = 0.00000970\n",
            "ite = 217   error = 0.00000964\n",
            "ite = 218   error = 0.00000957\n",
            "ite = 219   error = 0.00000951\n",
            "ite = 220   error = 0.00000946\n",
            "ite = 221   error = 0.00000940\n",
            "ite = 222   error = 0.00000934\n",
            "ite = 223   error = 0.00000929\n",
            "ite = 224   error = 0.00000924\n",
            "ite = 225   error = 0.00000919\n",
            "ite = 226   error = 0.00000914\n",
            "ite = 227   error = 0.00000909\n",
            "ite = 228   error = 0.00000904\n",
            "ite = 229   error = 0.00000900\n",
            "ite = 230   error = 0.00000896\n",
            "ite = 231   error = 0.00000891\n",
            "ite = 232   error = 0.00000887\n",
            "ite = 233   error = 0.00000883\n",
            "ite = 234   error = 0.00000880\n",
            "ite = 235   error = 0.00000876\n",
            "ite = 236   error = 0.00000872\n",
            "ite = 237   error = 0.00000869\n",
            "ite = 238   error = 0.00000866\n",
            "ite = 239   error = 0.00000863\n",
            "ite = 240   error = 0.00000859\n",
            "ite = 241   error = 0.00000857\n",
            "ite = 242   error = 0.00000854\n",
            "ite = 243   error = 0.00000851\n",
            "ite = 244   error = 0.00000848\n",
            "ite = 245   error = 0.00000846\n",
            "ite = 246   error = 0.00000844\n",
            "ite = 247   error = 0.00000841\n",
            "ite = 248   error = 0.00000839\n",
            "ite = 249   error = 0.00000837\n",
            "ite = 250   error = 0.00000835\n",
            "ite = 251   error = 0.00000833\n",
            "ite = 252   error = 0.00000832\n",
            "ite = 253   error = 0.00000830\n",
            "ite = 254   error = 0.00000829\n",
            "ite = 255   error = 0.00000827\n",
            "ite = 256   error = 0.00000826\n",
            "ite = 257   error = 0.00000825\n",
            "ite = 258   error = 0.00000823\n",
            "ite = 259   error = 0.00000822\n",
            "ite = 260   error = 0.00000821\n",
            "ite = 261   error = 0.00000821\n",
            "ite = 262   error = 0.00000820\n",
            "ite = 263   error = 0.00000819\n",
            "ite = 264   error = 0.00000818\n",
            "ite = 265   error = 0.00000818\n",
            "ite = 266   error = 0.00000818\n",
            "ite = 267   error = 0.00000817\n",
            "ite = 268   error = 0.00000817\n",
            "ite = 269   error = 0.00000817\n",
            "ite = 270   error = 0.00000817\n",
            "ite = 271   error = 0.00000817\n",
            "ite = 272   error = 0.00000817\n",
            "ite = 273   error = 0.00000817\n",
            "ite = 274   error = 0.00000817\n",
            "ite = 275   error = 0.00000817\n",
            "ite = 276   error = 0.00000818\n",
            "ite = 277   error = 0.00000818\n",
            "ite = 278   error = 0.00000818\n",
            "ite = 279   error = 0.00000819\n",
            "ite = 280   error = 0.00000820\n",
            "ite = 281   error = 0.00000820\n",
            "ite = 282   error = 0.00000821\n",
            "ite = 283   error = 0.00000822\n",
            "ite = 284   error = 0.00000823\n",
            "ite = 285   error = 0.00000824\n",
            "ite = 286   error = 0.00000825\n",
            "ite = 287   error = 0.00000826\n",
            "ite = 288   error = 0.00000827\n",
            "ite = 289   error = 0.00000829\n",
            "ite = 290   error = 0.00000830\n",
            "ite = 291   error = 0.00000831\n",
            "ite = 292   error = 0.00000833\n",
            "ite = 293   error = 0.00000834\n",
            "ite = 294   error = 0.00000836\n",
            "ite = 295   error = 0.00000837\n",
            "ite = 296   error = 0.00000839\n",
            "ite = 297   error = 0.00000841\n",
            "ite = 298   error = 0.00000843\n",
            "ite = 299   error = 0.00000845\n",
            "ite = 300   error = 0.00000847\n",
            "ite = 301   error = 0.00000849\n",
            "ite = 302   error = 0.00000851\n",
            "ite = 303   error = 0.00000853\n",
            "ite = 304   error = 0.00000855\n",
            "ite = 305   error = 0.00000857\n",
            "ite = 306   error = 0.00000859\n",
            "ite = 307   error = 0.00000862\n",
            "ite = 308   error = 0.00000864\n",
            "ite = 309   error = 0.00000867\n",
            "ite = 310   error = 0.00000869\n",
            "ite = 311   error = 0.00000872\n",
            "ite = 312   error = 0.00000874\n",
            "ite = 313   error = 0.00000877\n",
            "ite = 314   error = 0.00000880\n",
            "ite = 315   error = 0.00000883\n",
            "ite = 316   error = 0.00000885\n",
            "ite = 317   error = 0.00000888\n",
            "ite = 318   error = 0.00000891\n",
            "ite = 319   error = 0.00000894\n",
            "ite = 320   error = 0.00000897\n",
            "ite = 321   error = 0.00000901\n",
            "ite = 322   error = 0.00000904\n",
            "ite = 323   error = 0.00000907\n",
            "ite = 324   error = 0.00000910\n",
            "ite = 325   error = 0.00000914\n",
            "ite = 326   error = 0.00000917\n",
            "ite = 327   error = 0.00000920\n",
            "ite = 328   error = 0.00000924\n",
            "ite = 329   error = 0.00000927\n",
            "ite = 330   error = 0.00000931\n",
            "ite = 331   error = 0.00000935\n",
            "ite = 332   error = 0.00000938\n",
            "ite = 333   error = 0.00000942\n",
            "ite = 334   error = 0.00000946\n",
            "ite = 335   error = 0.00000950\n",
            "ite = 336   error = 0.00000954\n",
            "ite = 337   error = 0.00000958\n",
            "ite = 338   error = 0.00000962\n",
            "ite = 339   error = 0.00000966\n",
            "ite = 340   error = 0.00000970\n",
            "ite = 341   error = 0.00000974\n",
            "ite = 342   error = 0.00000979\n",
            "ite = 343   error = 0.00000983\n",
            "ite = 344   error = 0.00000987\n",
            "ite = 345   error = 0.00000992\n",
            "ite = 346   error = 0.00000996\n",
            "ite = 347   error = 0.00001001\n",
            "ite = 348   error = 0.00001006\n",
            "ite = 349   error = 0.00001010\n",
            "ite = 350   error = 0.00001015\n",
            "ite = 351   error = 0.00001020\n",
            "ite = 352   error = 0.00001025\n",
            "ite = 353   error = 0.00001029\n",
            "ite = 354   error = 0.00001034\n",
            "ite = 355   error = 0.00001039\n",
            "ite = 356   error = 0.00001045\n",
            "ite = 357   error = 0.00001050\n",
            "ite = 358   error = 0.00001055\n",
            "ite = 359   error = 0.00001060\n",
            "ite = 360   error = 0.00001065\n",
            "ite = 361   error = 0.00001071\n",
            "ite = 362   error = 0.00001076\n",
            "ite = 363   error = 0.00001082\n",
            "ite = 364   error = 0.00001087\n",
            "ite = 365   error = 0.00001093\n",
            "ite = 366   error = 0.00001099\n",
            "ite = 367   error = 0.00001104\n",
            "ite = 368   error = 0.00001110\n",
            "ite = 369   error = 0.00001116\n",
            "ite = 370   error = 0.00001122\n",
            "ite = 371   error = 0.00001128\n",
            "ite = 372   error = 0.00001134\n",
            "ite = 373   error = 0.00001140\n",
            "ite = 374   error = 0.00001146\n",
            "ite = 375   error = 0.00001153\n",
            "ite = 376   error = 0.00001159\n",
            "ite = 377   error = 0.00001165\n",
            "ite = 378   error = 0.00001172\n",
            "ite = 379   error = 0.00001178\n",
            "ite = 380   error = 0.00001185\n",
            "ite = 381   error = 0.00001192\n",
            "ite = 382   error = 0.00001198\n",
            "ite = 383   error = 0.00001205\n",
            "ite = 384   error = 0.00001212\n",
            "ite = 385   error = 0.00001219\n",
            "ite = 386   error = 0.00001226\n",
            "ite = 387   error = 0.00001233\n",
            "ite = 388   error = 0.00001240\n",
            "ite = 389   error = 0.00001248\n",
            "ite = 390   error = 0.00001255\n",
            "ite = 391   error = 0.00001262\n",
            "ite = 392   error = 0.00001270\n",
            "ite = 393   error = 0.00001277\n",
            "ite = 394   error = 0.00001285\n",
            "ite = 395   error = 0.00001293\n",
            "ite = 396   error = 0.00001300\n",
            "ite = 397   error = 0.00001308\n",
            "ite = 398   error = 0.00001316\n",
            "ite = 399   error = 0.00001324\n",
            "ite = 400   error = 0.00001332\n",
            "ite = 401   error = 0.00001340\n",
            "ite = 402   error = 0.00001349\n",
            "ite = 403   error = 0.00001357\n",
            "ite = 404   error = 0.00001365\n",
            "ite = 405   error = 0.00001374\n",
            "ite = 406   error = 0.00001382\n",
            "ite = 407   error = 0.00001391\n",
            "ite = 408   error = 0.00001400\n",
            "ite = 409   error = 0.00001408\n",
            "ite = 410   error = 0.00001417\n",
            "ite = 411   error = 0.00001426\n",
            "ite = 412   error = 0.00001435\n",
            "ite = 413   error = 0.00001444\n",
            "ite = 414   error = 0.00001453\n",
            "ite = 415   error = 0.00001463\n",
            "ite = 416   error = 0.00001472\n",
            "ite = 417   error = 0.00001482\n",
            "ite = 418   error = 0.00001491\n",
            "ite = 419   error = 0.00001501\n",
            "ite = 420   error = 0.00001510\n",
            "ite = 421   error = 0.00001520\n",
            "ite = 422   error = 0.00001530\n",
            "ite = 423   error = 0.00001540\n",
            "ite = 424   error = 0.00001550\n",
            "ite = 425   error = 0.00001560\n",
            "ite = 426   error = 0.00001570\n",
            "ite = 427   error = 0.00001580\n",
            "ite = 428   error = 0.00001591\n",
            "ite = 429   error = 0.00001601\n",
            "ite = 430   error = 0.00001612\n",
            "ite = 431   error = 0.00001622\n",
            "ite = 432   error = 0.00001633\n",
            "ite = 433   error = 0.00001643\n",
            "ite = 434   error = 0.00001654\n",
            "ite = 435   error = 0.00001665\n",
            "ite = 436   error = 0.00001676\n",
            "ite = 437   error = 0.00001687\n",
            "ite = 438   error = 0.00001698\n",
            "ite = 439   error = 0.00001710\n",
            "ite = 440   error = 0.00001721\n",
            "ite = 441   error = 0.00001732\n",
            "ite = 442   error = 0.00001744\n",
            "ite = 443   error = 0.00001755\n",
            "ite = 444   error = 0.00001767\n",
            "ite = 445   error = 0.00001778\n",
            "ite = 446   error = 0.00001790\n",
            "ite = 447   error = 0.00001802\n",
            "ite = 448   error = 0.00001814\n",
            "ite = 449   error = 0.00001826\n",
            "ite = 450   error = 0.00001838\n",
            "ite = 451   error = 0.00001850\n",
            "ite = 452   error = 0.00001862\n",
            "ite = 453   error = 0.00001874\n",
            "ite = 454   error = 0.00001886\n",
            "ite = 455   error = 0.00001899\n",
            "ite = 456   error = 0.00001911\n",
            "ite = 457   error = 0.00001923\n",
            "ite = 458   error = 0.00001936\n",
            "ite = 459   error = 0.00001948\n",
            "ite = 460   error = 0.00001961\n",
            "ite = 461   error = 0.00001974\n",
            "ite = 462   error = 0.00001986\n",
            "ite = 463   error = 0.00001999\n",
            "ite = 464   error = 0.00002012\n",
            "ite = 465   error = 0.00002025\n",
            "ite = 466   error = 0.00002037\n",
            "ite = 467   error = 0.00002050\n",
            "ite = 468   error = 0.00002063\n",
            "ite = 469   error = 0.00002076\n",
            "ite = 470   error = 0.00002089\n",
            "ite = 471   error = 0.00002102\n",
            "ite = 472   error = 0.00002115\n",
            "ite = 473   error = 0.00002128\n",
            "ite = 474   error = 0.00002142\n",
            "ite = 475   error = 0.00002155\n",
            "ite = 476   error = 0.00002168\n",
            "ite = 477   error = 0.00002181\n",
            "ite = 478   error = 0.00002194\n",
            "ite = 479   error = 0.00002207\n",
            "ite = 480   error = 0.00002220\n",
            "ite = 481   error = 0.00002234\n",
            "ite = 482   error = 0.00002247\n",
            "ite = 483   error = 0.00002260\n",
            "ite = 484   error = 0.00002273\n",
            "ite = 485   error = 0.00002286\n",
            "ite = 486   error = 0.00002300\n",
            "ite = 487   error = 0.00002313\n",
            "ite = 488   error = 0.00002326\n",
            "ite = 489   error = 0.00002339\n",
            "ite = 490   error = 0.00002352\n",
            "ite = 491   error = 0.00002365\n",
            "ite = 492   error = 0.00002378\n",
            "ite = 493   error = 0.00002391\n",
            "ite = 494   error = 0.00002404\n",
            "ite = 495   error = 0.00002417\n",
            "ite = 496   error = 0.00002430\n",
            "ite = 497   error = 0.00002442\n",
            "ite = 498   error = 0.00002455\n",
            "ite = 499   error = 0.00002468\n",
            "ite = 500   error = 0.00002480\n",
            "ite = 501   error = 0.00002493\n",
            "ite = 502   error = 0.00002505\n",
            "ite = 503   error = 0.00002518\n",
            "ite = 504   error = 0.00002530\n",
            "ite = 505   error = 0.00002543\n",
            "ite = 506   error = 0.00002555\n",
            "ite = 507   error = 0.00002567\n",
            "ite = 508   error = 0.00002579\n",
            "ite = 509   error = 0.00002591\n",
            "ite = 510   error = 0.00002603\n",
            "ite = 511   error = 0.00002614\n",
            "ite = 512   error = 0.00002626\n",
            "ite = 513   error = 0.00002638\n",
            "ite = 514   error = 0.00002649\n",
            "ite = 515   error = 0.00002661\n",
            "ite = 516   error = 0.00002672\n",
            "ite = 517   error = 0.00002683\n",
            "ite = 518   error = 0.00002694\n",
            "ite = 519   error = 0.00002705\n",
            "ite = 520   error = 0.00002716\n",
            "ite = 521   error = 0.00002726\n",
            "ite = 522   error = 0.00002737\n",
            "ite = 523   error = 0.00002747\n",
            "ite = 524   error = 0.00002757\n",
            "ite = 525   error = 0.00002768\n",
            "ite = 526   error = 0.00002778\n",
            "ite = 527   error = 0.00002787\n",
            "ite = 528   error = 0.00002797\n",
            "ite = 529   error = 0.00002807\n",
            "ite = 530   error = 0.00002816\n",
            "ite = 531   error = 0.00002826\n",
            "ite = 532   error = 0.00002835\n",
            "ite = 533   error = 0.00002844\n",
            "ite = 534   error = 0.00002853\n",
            "ite = 535   error = 0.00002861\n",
            "ite = 536   error = 0.00002870\n",
            "ite = 537   error = 0.00002878\n",
            "ite = 538   error = 0.00002886\n",
            "ite = 539   error = 0.00002895\n",
            "ite = 540   error = 0.00002903\n",
            "ite = 541   error = 0.00002910\n",
            "ite = 542   error = 0.00002918\n",
            "ite = 543   error = 0.00002925\n",
            "ite = 544   error = 0.00002933\n",
            "ite = 545   error = 0.00002940\n",
            "ite = 546   error = 0.00002947\n",
            "ite = 547   error = 0.00002954\n",
            "ite = 548   error = 0.00002960\n",
            "ite = 549   error = 0.00002967\n",
            "ite = 550   error = 0.00002973\n",
            "ite = 551   error = 0.00002979\n",
            "ite = 552   error = 0.00002985\n",
            "ite = 553   error = 0.00002991\n",
            "ite = 554   error = 0.00002997\n",
            "ite = 555   error = 0.00003002\n",
            "ite = 556   error = 0.00003008\n",
            "ite = 557   error = 0.00003013\n",
            "ite = 558   error = 0.00003018\n",
            "ite = 559   error = 0.00003023\n",
            "ite = 560   error = 0.00003027\n",
            "ite = 561   error = 0.00003032\n",
            "ite = 562   error = 0.00003036\n",
            "ite = 563   error = 0.00003041\n",
            "ite = 564   error = 0.00003045\n",
            "ite = 565   error = 0.00003049\n",
            "ite = 566   error = 0.00003052\n",
            "ite = 567   error = 0.00003056\n",
            "ite = 568   error = 0.00003060\n",
            "ite = 569   error = 0.00003063\n",
            "ite = 570   error = 0.00003066\n",
            "ite = 571   error = 0.00003069\n",
            "ite = 572   error = 0.00003072\n",
            "ite = 573   error = 0.00003075\n",
            "ite = 574   error = 0.00003077\n",
            "ite = 575   error = 0.00003080\n",
            "ite = 576   error = 0.00003082\n",
            "ite = 577   error = 0.00003084\n",
            "ite = 578   error = 0.00003086\n",
            "ite = 579   error = 0.00003088\n",
            "ite = 580   error = 0.00003090\n",
            "ite = 581   error = 0.00003092\n",
            "ite = 582   error = 0.00003093\n",
            "ite = 583   error = 0.00003095\n",
            "ite = 584   error = 0.00003096\n",
            "ite = 585   error = 0.00003097\n",
            "ite = 586   error = 0.00003098\n",
            "ite = 587   error = 0.00003099\n",
            "ite = 588   error = 0.00003100\n",
            "ite = 589   error = 0.00003100\n",
            "ite = 590   error = 0.00003101\n",
            "ite = 591   error = 0.00003101\n",
            "ite = 592   error = 0.00003102\n",
            "ite = 593   error = 0.00003102\n",
            "ite = 594   error = 0.00003102\n",
            "ite = 595   error = 0.00003102\n",
            "ite = 596   error = 0.00003102\n",
            "ite = 597   error = 0.00003102\n",
            "ite = 598   error = 0.00003102\n",
            "ite = 599   error = 0.00003101\n",
            "ite = 600   error = 0.00003101\n",
            "ite = 601   error = 0.00003100\n",
            "ite = 602   error = 0.00003100\n",
            "ite = 603   error = 0.00003099\n",
            "ite = 604   error = 0.00003098\n",
            "ite = 605   error = 0.00003097\n",
            "ite = 606   error = 0.00003096\n",
            "ite = 607   error = 0.00003095\n",
            "ite = 608   error = 0.00003094\n",
            "ite = 609   error = 0.00003093\n",
            "ite = 610   error = 0.00003092\n",
            "ite = 611   error = 0.00003090\n",
            "ite = 612   error = 0.00003089\n",
            "ite = 613   error = 0.00003088\n",
            "ite = 614   error = 0.00003086\n",
            "ite = 615   error = 0.00003085\n",
            "ite = 616   error = 0.00003083\n",
            "ite = 617   error = 0.00003081\n",
            "ite = 618   error = 0.00003080\n",
            "ite = 619   error = 0.00003078\n",
            "ite = 620   error = 0.00003076\n",
            "ite = 621   error = 0.00003074\n",
            "ite = 622   error = 0.00003072\n",
            "ite = 623   error = 0.00003070\n",
            "ite = 624   error = 0.00003068\n",
            "ite = 625   error = 0.00003066\n",
            "ite = 626   error = 0.00003064\n",
            "ite = 627   error = 0.00003062\n",
            "ite = 628   error = 0.00003060\n",
            "ite = 629   error = 0.00003058\n",
            "ite = 630   error = 0.00003056\n",
            "ite = 631   error = 0.00003054\n",
            "ite = 632   error = 0.00003052\n",
            "ite = 633   error = 0.00003049\n",
            "ite = 634   error = 0.00003047\n",
            "ite = 635   error = 0.00003045\n",
            "ite = 636   error = 0.00003043\n",
            "ite = 637   error = 0.00003040\n",
            "ite = 638   error = 0.00003038\n",
            "ite = 639   error = 0.00003036\n",
            "ite = 640   error = 0.00003034\n",
            "ite = 641   error = 0.00003031\n",
            "ite = 642   error = 0.00003029\n",
            "ite = 643   error = 0.00003027\n",
            "ite = 644   error = 0.00003024\n",
            "ite = 645   error = 0.00003022\n",
            "ite = 646   error = 0.00003020\n",
            "ite = 647   error = 0.00003017\n",
            "ite = 648   error = 0.00003015\n",
            "ite = 649   error = 0.00003013\n",
            "ite = 650   error = 0.00003011\n",
            "ite = 651   error = 0.00003008\n",
            "ite = 652   error = 0.00003006\n",
            "ite = 653   error = 0.00003004\n",
            "ite = 654   error = 0.00003002\n",
            "ite = 655   error = 0.00003000\n",
            "ite = 656   error = 0.00002997\n",
            "ite = 657   error = 0.00002995\n",
            "ite = 658   error = 0.00002993\n",
            "ite = 659   error = 0.00002991\n",
            "ite = 660   error = 0.00002989\n",
            "ite = 661   error = 0.00002987\n",
            "ite = 662   error = 0.00002985\n",
            "ite = 663   error = 0.00002983\n",
            "ite = 664   error = 0.00002981\n",
            "ite = 665   error = 0.00002979\n",
            "ite = 666   error = 0.00002977\n",
            "ite = 667   error = 0.00002975\n",
            "ite = 668   error = 0.00002973\n",
            "ite = 669   error = 0.00002971\n",
            "ite = 670   error = 0.00002970\n",
            "ite = 671   error = 0.00002968\n",
            "ite = 672   error = 0.00002966\n",
            "ite = 673   error = 0.00002965\n",
            "ite = 674   error = 0.00002963\n",
            "ite = 675   error = 0.00002962\n",
            "ite = 676   error = 0.00002960\n",
            "ite = 677   error = 0.00002959\n",
            "ite = 678   error = 0.00002957\n",
            "ite = 679   error = 0.00002956\n",
            "ite = 680   error = 0.00002954\n",
            "ite = 681   error = 0.00002953\n",
            "ite = 682   error = 0.00002952\n",
            "ite = 683   error = 0.00002951\n",
            "ite = 684   error = 0.00002950\n",
            "ite = 685   error = 0.00002949\n",
            "ite = 686   error = 0.00002948\n",
            "ite = 687   error = 0.00002947\n",
            "ite = 688   error = 0.00002946\n",
            "ite = 689   error = 0.00002945\n",
            "ite = 690   error = 0.00002944\n",
            "ite = 691   error = 0.00002944\n",
            "ite = 692   error = 0.00002943\n",
            "ite = 693   error = 0.00002942\n",
            "ite = 694   error = 0.00002942\n",
            "ite = 695   error = 0.00002941\n",
            "ite = 696   error = 0.00002941\n",
            "ite = 697   error = 0.00002941\n",
            "ite = 698   error = 0.00002941\n",
            "ite = 699   error = 0.00002940\n",
            "ite = 700   error = 0.00002940\n",
            "ite = 701   error = 0.00002940\n",
            "ite = 702   error = 0.00002940\n",
            "ite = 703   error = 0.00002940\n",
            "ite = 704   error = 0.00002941\n",
            "ite = 705   error = 0.00002941\n",
            "ite = 706   error = 0.00002941\n",
            "ite = 707   error = 0.00002942\n",
            "ite = 708   error = 0.00002942\n",
            "ite = 709   error = 0.00002943\n",
            "ite = 710   error = 0.00002943\n",
            "ite = 711   error = 0.00002944\n",
            "ite = 712   error = 0.00002945\n",
            "ite = 713   error = 0.00002946\n",
            "ite = 714   error = 0.00002947\n",
            "ite = 715   error = 0.00002948\n",
            "ite = 716   error = 0.00002949\n",
            "ite = 717   error = 0.00002951\n",
            "ite = 718   error = 0.00002952\n",
            "ite = 719   error = 0.00002953\n",
            "ite = 720   error = 0.00002955\n",
            "ite = 721   error = 0.00002957\n",
            "ite = 722   error = 0.00002958\n",
            "ite = 723   error = 0.00002960\n",
            "ite = 724   error = 0.00002962\n",
            "ite = 725   error = 0.00002964\n",
            "ite = 726   error = 0.00002966\n",
            "ite = 727   error = 0.00002968\n",
            "ite = 728   error = 0.00002971\n",
            "ite = 729   error = 0.00002973\n",
            "ite = 730   error = 0.00002976\n",
            "ite = 731   error = 0.00002978\n",
            "ite = 732   error = 0.00002981\n",
            "ite = 733   error = 0.00002984\n",
            "ite = 734   error = 0.00002987\n",
            "ite = 735   error = 0.00002990\n",
            "ite = 736   error = 0.00002993\n",
            "ite = 737   error = 0.00002996\n",
            "ite = 738   error = 0.00003000\n",
            "ite = 739   error = 0.00003003\n",
            "ite = 740   error = 0.00003007\n",
            "ite = 741   error = 0.00003011\n",
            "ite = 742   error = 0.00003015\n",
            "ite = 743   error = 0.00003019\n",
            "ite = 744   error = 0.00003023\n",
            "ite = 745   error = 0.00003027\n",
            "ite = 746   error = 0.00003031\n",
            "ite = 747   error = 0.00003036\n",
            "ite = 748   error = 0.00003040\n",
            "ite = 749   error = 0.00003045\n",
            "ite = 750   error = 0.00003050\n",
            "ite = 751   error = 0.00003055\n",
            "ite = 752   error = 0.00003060\n",
            "ite = 753   error = 0.00003066\n",
            "ite = 754   error = 0.00003071\n",
            "ite = 755   error = 0.00003077\n",
            "ite = 756   error = 0.00003082\n",
            "ite = 757   error = 0.00003088\n",
            "ite = 758   error = 0.00003094\n",
            "ite = 759   error = 0.00003100\n",
            "ite = 760   error = 0.00003107\n",
            "ite = 761   error = 0.00003113\n",
            "ite = 762   error = 0.00003120\n",
            "ite = 763   error = 0.00003127\n",
            "ite = 764   error = 0.00003134\n",
            "ite = 765   error = 0.00003141\n",
            "ite = 766   error = 0.00003148\n",
            "ite = 767   error = 0.00003156\n",
            "ite = 768   error = 0.00003163\n",
            "ite = 769   error = 0.00003171\n",
            "ite = 770   error = 0.00003179\n",
            "ite = 771   error = 0.00003187\n",
            "ite = 772   error = 0.00003196\n",
            "ite = 773   error = 0.00003204\n",
            "ite = 774   error = 0.00003213\n",
            "ite = 775   error = 0.00003222\n",
            "ite = 776   error = 0.00003231\n",
            "ite = 777   error = 0.00003240\n",
            "ite = 778   error = 0.00003250\n",
            "ite = 779   error = 0.00003259\n",
            "ite = 780   error = 0.00003269\n",
            "ite = 781   error = 0.00003279\n",
            "ite = 782   error = 0.00003290\n",
            "ite = 783   error = 0.00003300\n",
            "ite = 784   error = 0.00003311\n",
            "ite = 785   error = 0.00003322\n",
            "ite = 786   error = 0.00003333\n",
            "ite = 787   error = 0.00003345\n",
            "ite = 788   error = 0.00003356\n",
            "ite = 789   error = 0.00003368\n",
            "ite = 790   error = 0.00003380\n",
            "ite = 791   error = 0.00003393\n",
            "ite = 792   error = 0.00003405\n",
            "ite = 793   error = 0.00003418\n",
            "ite = 794   error = 0.00003431\n",
            "ite = 795   error = 0.00003445\n",
            "ite = 796   error = 0.00003458\n",
            "ite = 797   error = 0.00003472\n",
            "ite = 798   error = 0.00003487\n",
            "ite = 799   error = 0.00003501\n",
            "ite = 800   error = 0.00003516\n",
            "% aciertos X_train : 0.891\n",
            "Confusion matrix:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84        52\n",
            "           1       0.96      0.92      0.94        53\n",
            "           2       0.83      0.95      0.89        42\n",
            "\n",
            "    accuracy                           0.89       147\n",
            "   macro avg       0.89      0.89      0.89       147\n",
            "weighted avg       0.89      0.89      0.89       147\n",
            "\n",
            "Confusion matrix:\n",
            "[[42  2  8]\n",
            " [ 4 49  0]\n",
            " [ 2  0 40]]\n",
            "% aciertos X_test : 0.921\n",
            "Confusion matrix:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86        18\n",
            "           1       1.00      0.94      0.97        17\n",
            "           2       0.93      0.93      0.93        28\n",
            "\n",
            "    accuracy                           0.92        63\n",
            "   macro avg       0.92      0.92      0.92        63\n",
            "weighted avg       0.92      0.92      0.92        63\n",
            "\n",
            "Confusion matrix:\n",
            "[[16  0  2]\n",
            " [ 1 16  0]\n",
            " [ 2  0 26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5W5X8D2Xyvqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 3**"
      ],
      "metadata": {
        "id": "RmboJDBEz_D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*El archivo Vinos.csv tiene información referida a 13 características químicas y/o visuales de varias muestras de vinos pertenecientes a 3 clases distintas.*\n",
        "\n",
        "*Utilice el 80% de los ejemplos del archivo Vinos.csv para entrenar un multiperceptrón que sea capaz que distinguir entre las 3 clases de vinos. Observe la tasa de acierto obtenida sobre el 20% restante.*"
      ],
      "metadata": {
        "id": "8p1vM1KY2NlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "df = pd.read_csv('Vinos.csv', sep=';', encoding='utf-8')\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "HRJe4JIy2XI-",
        "outputId": "12d099fa-9e40-4b8b-ed23-c1a61123aa1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
              "0        1    14.23        1.71  2.43               15.6        127   \n",
              "1        1    13.20        1.78  2.14               11.2        100   \n",
              "2        1    13.16        2.36  2.67               18.6        101   \n",
              "3        1    14.37        1.95  2.50               16.8        113   \n",
              "4        1    13.24        2.59  2.87               21.0        118   \n",
              "..     ...      ...         ...   ...                ...        ...   \n",
              "173      3    13.71        5.65  2.45               20.5         95   \n",
              "174      3    13.40        3.91  2.48               23.0        102   \n",
              "175      3    13.27        4.28  2.26               20.0        120   \n",
              "176      3    13.17        2.59  2.37               20.0        120   \n",
              "177      3    14.13        4.10  2.74               24.5         96   \n",
              "\n",
              "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
              "0             2.80        3.06                  0.28             2.29   \n",
              "1             2.65        2.76                  0.26             1.28   \n",
              "2             2.80        3.24                  0.30             2.81   \n",
              "3             3.85        3.49                  0.24             2.18   \n",
              "4             2.80        2.69                  0.39             1.82   \n",
              "..             ...         ...                   ...              ...   \n",
              "173           1.68        0.61                  0.52             1.06   \n",
              "174           1.80        0.75                  0.43             1.41   \n",
              "175           1.59        0.69                  0.43             1.35   \n",
              "176           1.65        0.68                  0.53             1.46   \n",
              "177           2.05        0.76                  0.56             1.35   \n",
              "\n",
              "     Color intensity   Hue  OD280/OD315  Proline  \n",
              "0               5.64  1.04         3.92     1065  \n",
              "1               4.38  1.05         3.40     1050  \n",
              "2               5.68  1.03         3.17     1185  \n",
              "3               7.80  0.86         3.45     1480  \n",
              "4               4.32  1.04         2.93      735  \n",
              "..               ...   ...          ...      ...  \n",
              "173             7.70  0.64         1.74      740  \n",
              "174             7.30  0.70         1.56      750  \n",
              "175            10.20  0.59         1.56      835  \n",
              "176             9.30  0.60         1.62      840  \n",
              "177             9.20  0.61         1.60      560  \n",
              "\n",
              "[178 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divido datos del df\n",
        "# random_state hace que se obtenga el mismo subconjunto de entrenamiento\n",
        "# cada vez que se ejecute el código con el mismo valor de random_state\n",
        "X_train = df.sample(frac = 0.8, random_state = 0) # contiene 80% de los datos\n",
        "x_test = df.drop(X_train.index) # contiene el 20% restante de los datos\n",
        "\n",
        "# rtas esperadas: clases\n",
        "Y = np.array(X_train.iloc[:,0]) # selecciona filas de primera columna del df --> clase\n",
        "\n",
        "# valores de entradas\n",
        "X = X_train.iloc[:, 1:] # contiene de la columa 1 todos los datos\n",
        "X = np.array(X) # los pone en un array\n",
        "\n",
        "\n",
        "# valores con los que testeo\n",
        "X_test = np.array(x_test.iloc[:, 1:])\n",
        "Y_test = np.array(x_test.iloc[:, 0])\n",
        "\n",
        "\n",
        "# normalizo entrada\n",
        "normalizarEntrada = 1\n",
        "if normalizarEntrada:\n",
        "  # Escala valores entre 0 y 1\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "  # X_train = min_max_scaler.fit_transform(X_train)\n",
        "   # X_test = min_max_scaler.transform(X_test)\n",
        "\n",
        "print(\"Cant datos de entrenamiento: \", len(X))\n",
        "print(\"cant de datos testeo: \", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftmg39ar31Nt",
        "outputId": "b50d1a67-9a99-45be-c610-fc2ca0b32303"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cant datos de entrenamiento:  142\n",
            "cant de datos testeo:  36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entradas = X.shape[1] # num de columnas del arreglo --> 13\n",
        "#print (entradas)\n",
        "ocultas = 12\n",
        "salidas = Y.shape[0]  # num de elementos en la dimension 0 --> 142\n",
        "#print(salidas)\n",
        "\n",
        "alfa = 0.001\n",
        "MAX_ITE = 1000\n",
        "\n",
        "FunH = 'identity'\n",
        "\n",
        "# Creación de modelo\n",
        "modelo = MLPClassifier(max_iter=MAX_ITE, hidden_layer_sizes=ocultas, alpha=alfa,\n",
        "                       solver='sgd', activation=FunH, tol=0.001,\n",
        "                       verbose=False)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "modelo.out_activation_ = 'softmax'\n",
        "\n",
        "# Medición del entrenamiento\n",
        "Y_pred = modelo.predict(X_test)\n",
        "score = modelo.score(X_test, Y_test)\n",
        "\n",
        "# imprimo rtas\n",
        "print(Y_pred)\n",
        "print(Y_test)\n",
        "\n",
        "\n",
        "# calculo manual del accuracy\n",
        "print('Efectividad: %6.2f%%' % (100*(Y_pred == Y_test).sum()/len(Y_test)) )\n",
        "print('      Score: %6.2f%%' % (score) )\n",
        "\n",
        "\n",
        "# Y_pred == Y_test --> devuelve true si la predic es correcta\n",
        "# .sum --> suma el total de true de la operación anterior"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wurNcEeS4JQ5",
        "outputId": "50d8ecf8-c630-4904-d332-81106b18b8c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "142\n",
            "[1 1 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 3 3 3 2 2 2 3 3 3 3]\n",
            "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "Efectividad:  83.33%\n",
            "      Score:   0.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0m-Y6S_vmltI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRqs0EpM274v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}