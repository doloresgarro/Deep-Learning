{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0DfKXb96Hy+TjyVHszCoV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/doloresgarro/Deep-Learning/blob/main/Pr%C3%A1ctica4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Práctica 4 - Deep Learning**\n",
        "*Multiperceptrón*"
      ],
      "metadata": {
        "id": "a6oJMbf-Y4AP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 1**"
      ],
      "metadata": {
        "id": "_N2Ryy2jYz4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*a) En base a esta información, indique:*\n",
        "\n",
        "*▪ Cuántos ejemplos se utilizaron en el entrenamiento.*\n",
        "\n",
        "\n",
        "*▪ Cuántas clases puede reconocer este multiperceptrón.*\n",
        "\n",
        "*▪ Cuál es la precisión (accuracy) de la red sobre el conjunto de ejemplos completo.*\n",
        "\n",
        "*▪ Cuáles son los valores de precisión de la red al responder por cada uno de los valores de clase (precision).*\n",
        "\n",
        "*▪ Cuáles son los valores de sensibilidad de la red al responder por cada uno de los valores de clase (recall).*"
      ],
      "metadata": {
        "id": "dfxR5nDUZgIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Se entreno una red neuronal multiperceptrón para resolver un problema de clasificación y al medir su desempeño sobre el conjunto de datos de entrenamiento se obtuvo la siguiente matriz de confusión:*"
      ],
      "metadata": {
        "id": "0q_lh8A2aIDq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWS_CfR4Opou"
      },
      "outputs": [],
      "source": [
        "import pandas as pd      # para trabajar con archivos de datos csv, excel, etc: https://pandas.pydata.org/docs/getting_started/tutorials.html\n",
        "import numpy as np\n",
        "\n",
        "# calcula las metricas precision, recall, f1-score y accuracy a partir de la matriz de confusion\n",
        "# retorna tupla: ( precision, recall, f1_score, accuracy )\n",
        "def calcular_metricas(conf_mat):\n",
        "    precision = np.zeros(conf_mat.shape[0])  # inicializa en 0 el vector precisión, conf_mat.shape[0] --> indica el num de filas de la matriz con_mat\n",
        "                                             # por lo tanto precisión va a ser una vector inicializado en 0 con el num de filas de la matriz recibida como parámetro\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        precision[i] = conf_mat[i][i]/sum(conf_mat.T[i])  # en c/ pos divide el valor de la matriz en esa f y c por la suma de todos los elem de la matriz en la columna i\n",
        "\n",
        "    recall = np.zeros(conf_mat.shape[0])   # recall lo mismo q lo anterior\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        recall[i] = conf_mat[i][i]/sum(conf_mat[i])\n",
        "\n",
        "    f1_score = 2* (precision*recall) /(precision+recall)\n",
        "\n",
        "    accuracy =  0\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        accuracy+=conf_mat[i][i]\n",
        "    accuracy/= conf_mat.sum()        # conf_mat.sum() --> suma de elems de la matriz conf_mat\n",
        "\n",
        "    return ( precision, recall, f1_score, accuracy )\n",
        "\n",
        "# el parámetro metricas es una tupla ( precision, recall, f1_score, accuracy )\n",
        "def imprimir_metricas( metricas ):\n",
        "    (precision, recall, f1_score, accuracy) = metricas\n",
        "    print('\\n clase   precision    recall    f1-score')\n",
        "    for i in range(0, len(precision)):\n",
        "        print('%5d %10.2f %10.2f %10.2f' % (i, precision[i], recall[i], f1_score[i]))\n",
        "    print('\\naccuracy: %6.2f\\n' % accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "confusion = np.array([\n",
        "           [ 17, 0, 1, 0, 1 ],\n",
        "           [ 0, 12, 0, 0, 0 ],\n",
        "           [ 0, 0, 12, 0, 0 ],\n",
        "           [ 2, 0, 0, 38, 0 ],\n",
        "           [ 0, 8, 0, 0, 61 ]\n",
        "           ])\n",
        "\n",
        "print('\\n Matriz de Confusión:')\n",
        "print(confusion, '\\n')\n",
        "\n",
        "\n",
        "\n",
        "#calcula métricas de forma manual\n",
        "metricas = calcular_metricas(confusion)\n",
        "imprimir_metricas(metricas)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfnoh6PSZRDg",
        "outputId": "f0831a67-dde1-4796-a6d1-b5432be785a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Matriz de Confusión:\n",
            "[[17  0  1  0  1]\n",
            " [ 0 12  0  0  0]\n",
            " [ 0  0 12  0  0]\n",
            " [ 2  0  0 38  0]\n",
            " [ 0  8  0  0 61]] \n",
            "\n",
            "\n",
            " clase   precision    recall    f1-score\n",
            "    0       0.89       0.89       0.89\n",
            "    1       0.60       1.00       0.75\n",
            "    2       0.92       1.00       0.96\n",
            "    3       1.00       0.95       0.97\n",
            "    4       0.98       0.88       0.93\n",
            "\n",
            "accuracy:   0.92\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisión --> proporción de predicciones correctas sobre una clase\n",
        "\n",
        "Recall --> proporción de ejemplos de una clase que correctamente clasificados\n",
        "\n",
        "Acurrancy --> cantidad de aciertos sobre el total de ejemplos\n"
      ],
      "metadata": {
        "id": "IuSBcQ97gxPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) La clase con mejor valor de F1-score es la clase 3"
      ],
      "metadata": {
        "id": "FvALAnd8hnYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 2**"
      ],
      "metadata": {
        "id": "_qz0tBvHo_89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Se desea utilizar una red multiperceptrón para reconocer muestras de tres variedades diferentes de trigo:*\n",
        "\n",
        "*Kama, Rosa y Canadiense.*\n",
        "\n",
        "*Para entrenarla se utilizará una parte de los ejemplos del archivo\n",
        "SEMILLAS.CSV. Estos datos fueron utilizados en el ejercicio 3 la práctica 2.*\n",
        "\n",
        "\n",
        "\n",
        "*a) Con respecto a la arquitectura, indique:*\n",
        "\n",
        "*▪ La cantidad de neuronas de la capa de entrada.*\n",
        "\n",
        "\n",
        "*▪ La cantidad de neuronas de la capa de salida.*\n",
        "\n",
        "*▪ La cantidad de pesos (arcos) que tiene la red si se utiliza una única capa oculta formada por 4 neuronas*"
      ],
      "metadata": {
        "id": "lTvT60PMpEqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a)\n",
        "- entrada: 7 + bias = 8\n",
        "- salida: 3 porque tengo tres variedades de semillas\n",
        "- capa oculta: 4 neuronas\n",
        "- arcos:\n",
        "\n",
        "n⋅h1​+∑​hi​⋅hi+1​+hN​⋅m\n",
        "\n",
        "\n",
        "\n",
        "*   n --> neuronas en capa de entrada\n",
        "*   m --> neuronas en capa de salida\n",
        "*   hi --> neuronas en capa oculta\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pTSocv21xvdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import grafica as gr\n",
        "from sklearn import preprocessing, metrics, model_selection\n",
        "\n",
        "df = pd.read_csv('Semillas.csv')\n",
        "\n",
        "X = np.array(df.iloc[:, 0:4])\n",
        "nEj = X.shape[0] # cantidad de ejemplos (muestras) en los datos\n",
        "\n",
        "#column_names = df.columns\n",
        "#print(column_names)\n",
        "\n",
        "nomClases = pd.unique(df['Clase'])\n",
        "#-- la red tendrá una salida para cada tipo de flor\n",
        "salidas = len(nomClases)\n",
        "\n",
        "#-- la salida debe ser numérica --\n",
        "# transforma las clases en df['Clase'] en valores numéricos basados\n",
        "# en su posición en el arreglo nomClases.\n",
        "clase = df['Clase']\n",
        "Y=np.zeros(nEj)\n",
        "for s in range(nEj):\n",
        "    Y[s]=np.argwhere(nomClases == clase[s])\n",
        "Y = Y.astype(int)\n",
        "\n",
        "\n",
        "#--- CONJUNTOS DE ENTRENAMIENTO Y TESTEO ---\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split( \\\n",
        "        X, Y, test_size=0.30) #, random_state=42)\n",
        "\n",
        "Y_trainB = np.zeros((len(Y_train), salidas))\n",
        "for o in range(len(Y_train)):\n",
        "    Y_trainB[o, Y_train[o]]=1\n",
        "\n",
        "normalizarEntrada = 1  # 1 si normaliza; 0 si no\n",
        "\n",
        "if normalizarEntrada:\n",
        "    # Escala los valores entre 0 y 1\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "    # se aplica la normalización a los conjuntos de datos X_train y X_test\n",
        "    X_train = min_max_scaler.fit_transform(X_train)\n",
        "    X_test = min_max_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "entradas = X_train.shape[1] # obtiene las entradas de la red\n",
        "ocultas = 4 # neuronas en la capa oculta\n",
        "salidas = Y_trainB.shape[1] # cantidad de clases a predecir (3)\n",
        "print(entradas)\n",
        "print(ocultas)\n",
        "print(salidas)\n",
        "\n",
        "# inicializo pesos y bias\n",
        "# se inicializa W1 como matriz de pesos de la capa oculta\n",
        "# se inicializa W2 como matriz de pesos de la capa de salida\n",
        "# pesos y bias --> se inicializan aleatoriamente entre -0.5 y 0.5\n",
        "# los w se van a ir ajustando durante el entrenamiento\n",
        "W1 = np.random.uniform(-0.5,0.5,[ocultas, entradas])\n",
        "b1 = np.random.uniform(-0.5,0.5, [ocultas,1]) # 1 --> indica una única neurona\n",
        "W2 = np.random.uniform(-0.5,0.5,[salidas, ocultas])\n",
        "b2 = np.random.uniform(-0.5,0.5, [salidas,1])\n",
        "\n",
        "#=====  Calcular el error actual =====\n",
        "FunH = 'logsig'\n",
        "FunO = 'tansig'\n",
        "if (FunO == 'tansig'):\n",
        "    Y_trainB = 2 * Y_trainB - 1\n",
        "# capa oculta\n",
        "#FunH = 'tanh' # mapea entradas al rango [-1,1]\n",
        "# capa de salida\n",
        "#FunO = 'sigmoid' # mapea entradas al rango [0,1]\n",
        "#if (FunO == 'sigmoid'):\n",
        "#    Y_trainB = 2 * Y_trainB - 1 # --> ajusta etiquetas de clase al rango [-1,1]\n",
        "\n",
        "# --- Calcular la rta.de la red para TODOS los ejemplos ---\n",
        "# w1 --> matriz de pesos que conecta las neuronas de capa oculta con las caract de entrada\n",
        "# X_train --> matriz de caract de entrada. Num de clases de entrada\n",
        "# b1 --> biass\n",
        "NetasH = W1 @ X_train.T + b1\n",
        "\n",
        "# cantEj = P.shape[0]\n",
        "# netasH = np.zeros([ocultas, cantEj])\n",
        "# for i in range(cantEj):\n",
        "#     for o in range(ocultas):\n",
        "#         netasH[o,i]=b1[o]\n",
        "#         for e in range(entradas):\n",
        "#             netasH[o,i] = netasH[o,i] + W1[o,e]*P[i,e]\n",
        "\n",
        "# cálculo de salidas de la red para cada capa\n",
        "SalidasH = gr.evaluar(FunH, NetasH) # salidas de capa oculta\n",
        "NetasO = W2 @ SalidasH + b2 # entradas de la capa de salidda\n",
        "SalidasO = gr.evaluar(FunO, NetasO) # salidas de la capa de salida\n",
        "\n",
        "# -- calcular el error --\n",
        "AVGError = np.mean((Y_trainB.T - SalidasO)**2)\n",
        "\n",
        "alfa = 0.1\n",
        "CotaError = 1.0e-15\n",
        "MAX_ITERA = 800\n",
        "ite = 0\n",
        "errorAnt = 1\n",
        "while ( abs(AVGError-errorAnt) > CotaError ) and ( ite < MAX_ITERA ):\n",
        "    errorAnt = AVGError\n",
        "    for p in range(len(X_train)):   #para cada ejemplo\n",
        "        # propagar el ejemplo hacia adelante\n",
        "        netasH = W1 @ X_train[p:p+1,:].T + b1\n",
        "        salidasH = gr.evaluar(FunH, netasH)\n",
        "        netasO = W2 @ salidasH + b2\n",
        "        salidasO = gr.evaluar(FunO, netasO)\n",
        "\n",
        "        # calcular los errores en ambas capas\n",
        "        ErrorSalida = Y_trainB[p:p+1,:].T-salidasO\n",
        "        deltaO = ErrorSalida * gr.evaluarDerivada(FunO,salidasO)\n",
        "        deltaH = gr.evaluarDerivada(FunH,salidasH)*(W2.T @ deltaO)\n",
        "\n",
        "        # corregir todos los pesos\n",
        "        W1 = W1 + alfa * deltaH @ X_train[p:p+1,:]\n",
        "        b1 = b1 + alfa * deltaH\n",
        "        W2 = W2 + alfa * deltaO @ salidasH.T\n",
        "        b2 = b2 + alfa * deltaO\n",
        "\n",
        "    # Recalcular AVGError\n",
        "    NetasH = W1 @ X_train.T + b1\n",
        "    SalidasH = gr.evaluar(FunH, NetasH)\n",
        "    NetasO = W2 @ SalidasH + b2\n",
        "    SalidasO = gr.evaluar(FunO, NetasO)\n",
        "    AVGError = np.mean((Y_trainB.T - SalidasO)**2)\n",
        "\n",
        "    ite = ite + 1\n",
        "    print(\"ite = %3d   error = %.8f\" % (ite, abs(AVGError-errorAnt)))\n",
        "\n",
        "if (FunO == 'tansig'):\n",
        "    y_pred = 2*((SalidasO>0) * 1)-1\n",
        "\n",
        "if (FunO == 'logsig'):\n",
        "    y_pred = (SalidasO>0.5) * 1\n",
        "\n",
        "Y_pred = np.argmax(y_pred,axis=0)\n",
        "#metrics.accuracy_score(y_test,evaluar(FUN,pred_test))\n",
        "\n",
        "print(\"%% aciertos X_train : %.3f\" % metrics.accuracy_score(Y_train,Y_pred))\n",
        "\n",
        "report = metrics.classification_report(Y_train,Y_pred)\n",
        "print(\"Confusion matrix:\\n%s\" % report)\n",
        "MM = metrics.confusion_matrix(Y_train,Y_pred)\n",
        "print(\"Confusion matrix:\\n%s\" % MM)\n",
        "\n",
        "#--- aplicando la red a los datos de testeo ---\n",
        "NetasH = W1 @ X_test.T + b1\n",
        "SalidasH = gr.evaluar(FunH, NetasH)\n",
        "NetasO = W2 @ SalidasH + b2\n",
        "SalidasO = gr.evaluar(FunO, NetasO)\n",
        "\n",
        "if (FunO == 'tansig'):\n",
        "    y_predTest = 2*((SalidasO>0) * 1)-1\n",
        "\n",
        "if (FunO == 'logsig'):\n",
        "    y_predTest = (SalidasO>0.5) * 1\n",
        "\n",
        "Y_predTest = np.argmax(y_predTest,axis=0)\n",
        "\n",
        "print(\"%% aciertos X_test : %.3f\" % metrics.accuracy_score(Y_test,Y_predTest))\n",
        "\n",
        "report = metrics.classification_report(Y_test,Y_predTest)\n",
        "print(\"Confusion matrix:\\n%s\" % report)\n",
        "MM = metrics.confusion_matrix(Y_test,Y_predTest)\n",
        "print(\"Confusion matrix:\\n%s\" % MM)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZUL9R78Est3",
        "outputId": "521d4c54-ffd4-41c6-d720-91654cdaaba3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "4\n",
            "3\n",
            "ite =   1   error = 0.42461782\n",
            "ite =   2   error = 0.13363024\n",
            "ite =   3   error = 0.16860556\n",
            "ite =   4   error = 0.07329339\n",
            "ite =   5   error = 0.03859649\n",
            "ite =   6   error = 0.03153529\n",
            "ite =   7   error = 0.03124275\n",
            "ite =   8   error = 0.02982490\n",
            "ite =   9   error = 0.02556523\n",
            "ite =  10   error = 0.02019221\n",
            "ite =  11   error = 0.01533402\n",
            "ite =  12   error = 0.01152827\n",
            "ite =  13   error = 0.00872892\n",
            "ite =  14   error = 0.00672816\n",
            "ite =  15   error = 0.00531679\n",
            "ite =  16   error = 0.00432706\n",
            "ite =  17   error = 0.00363431\n",
            "ite =  18   error = 0.00314778\n",
            "ite =  19   error = 0.00280135\n",
            "ite =  20   error = 0.00254663\n",
            "ite =  21   error = 0.00234850\n",
            "ite =  22   error = 0.00218233\n",
            "ite =  23   error = 0.00203207\n",
            "ite =  24   error = 0.00188849\n",
            "ite =  25   error = 0.00174752\n",
            "ite =  26   error = 0.00160846\n",
            "ite =  27   error = 0.00147249\n",
            "ite =  28   error = 0.00134149\n",
            "ite =  29   error = 0.00121733\n",
            "ite =  30   error = 0.00110142\n",
            "ite =  31   error = 0.00099467\n",
            "ite =  32   error = 0.00089741\n",
            "ite =  33   error = 0.00080957\n",
            "ite =  34   error = 0.00073077\n",
            "ite =  35   error = 0.00066043\n",
            "ite =  36   error = 0.00059787\n",
            "ite =  37   error = 0.00054235\n",
            "ite =  38   error = 0.00049315\n",
            "ite =  39   error = 0.00044959\n",
            "ite =  40   error = 0.00041102\n",
            "ite =  41   error = 0.00037685\n",
            "ite =  42   error = 0.00034656\n",
            "ite =  43   error = 0.00031966\n",
            "ite =  44   error = 0.00029576\n",
            "ite =  45   error = 0.00027446\n",
            "ite =  46   error = 0.00025546\n",
            "ite =  47   error = 0.00023846\n",
            "ite =  48   error = 0.00022323\n",
            "ite =  49   error = 0.00020953\n",
            "ite =  50   error = 0.00019719\n",
            "ite =  51   error = 0.00018603\n",
            "ite =  52   error = 0.00017592\n",
            "ite =  53   error = 0.00016673\n",
            "ite =  54   error = 0.00015834\n",
            "ite =  55   error = 0.00015068\n",
            "ite =  56   error = 0.00014365\n",
            "ite =  57   error = 0.00013719\n",
            "ite =  58   error = 0.00013123\n",
            "ite =  59   error = 0.00012572\n",
            "ite =  60   error = 0.00012061\n",
            "ite =  61   error = 0.00011587\n",
            "ite =  62   error = 0.00011145\n",
            "ite =  63   error = 0.00010733\n",
            "ite =  64   error = 0.00010348\n",
            "ite =  65   error = 0.00009987\n",
            "ite =  66   error = 0.00009649\n",
            "ite =  67   error = 0.00009331\n",
            "ite =  68   error = 0.00009032\n",
            "ite =  69   error = 0.00008750\n",
            "ite =  70   error = 0.00008485\n",
            "ite =  71   error = 0.00008234\n",
            "ite =  72   error = 0.00007996\n",
            "ite =  73   error = 0.00007771\n",
            "ite =  74   error = 0.00007557\n",
            "ite =  75   error = 0.00007355\n",
            "ite =  76   error = 0.00007162\n",
            "ite =  77   error = 0.00006978\n",
            "ite =  78   error = 0.00006802\n",
            "ite =  79   error = 0.00006635\n",
            "ite =  80   error = 0.00006475\n",
            "ite =  81   error = 0.00006322\n",
            "ite =  82   error = 0.00006175\n",
            "ite =  83   error = 0.00006033\n",
            "ite =  84   error = 0.00005898\n",
            "ite =  85   error = 0.00005768\n",
            "ite =  86   error = 0.00005642\n",
            "ite =  87   error = 0.00005521\n",
            "ite =  88   error = 0.00005404\n",
            "ite =  89   error = 0.00005291\n",
            "ite =  90   error = 0.00005182\n",
            "ite =  91   error = 0.00005077\n",
            "ite =  92   error = 0.00004975\n",
            "ite =  93   error = 0.00004876\n",
            "ite =  94   error = 0.00004781\n",
            "ite =  95   error = 0.00004688\n",
            "ite =  96   error = 0.00004598\n",
            "ite =  97   error = 0.00004510\n",
            "ite =  98   error = 0.00004425\n",
            "ite =  99   error = 0.00004342\n",
            "ite = 100   error = 0.00004262\n",
            "ite = 101   error = 0.00004184\n",
            "ite = 102   error = 0.00004108\n",
            "ite = 103   error = 0.00004034\n",
            "ite = 104   error = 0.00003962\n",
            "ite = 105   error = 0.00003892\n",
            "ite = 106   error = 0.00003823\n",
            "ite = 107   error = 0.00003756\n",
            "ite = 108   error = 0.00003691\n",
            "ite = 109   error = 0.00003628\n",
            "ite = 110   error = 0.00003566\n",
            "ite = 111   error = 0.00003505\n",
            "ite = 112   error = 0.00003446\n",
            "ite = 113   error = 0.00003389\n",
            "ite = 114   error = 0.00003332\n",
            "ite = 115   error = 0.00003277\n",
            "ite = 116   error = 0.00003223\n",
            "ite = 117   error = 0.00003171\n",
            "ite = 118   error = 0.00003119\n",
            "ite = 119   error = 0.00003069\n",
            "ite = 120   error = 0.00003020\n",
            "ite = 121   error = 0.00002972\n",
            "ite = 122   error = 0.00002925\n",
            "ite = 123   error = 0.00002879\n",
            "ite = 124   error = 0.00002833\n",
            "ite = 125   error = 0.00002789\n",
            "ite = 126   error = 0.00002746\n",
            "ite = 127   error = 0.00002704\n",
            "ite = 128   error = 0.00002662\n",
            "ite = 129   error = 0.00002622\n",
            "ite = 130   error = 0.00002582\n",
            "ite = 131   error = 0.00002543\n",
            "ite = 132   error = 0.00002505\n",
            "ite = 133   error = 0.00002468\n",
            "ite = 134   error = 0.00002431\n",
            "ite = 135   error = 0.00002395\n",
            "ite = 136   error = 0.00002360\n",
            "ite = 137   error = 0.00002326\n",
            "ite = 138   error = 0.00002292\n",
            "ite = 139   error = 0.00002259\n",
            "ite = 140   error = 0.00002226\n",
            "ite = 141   error = 0.00002194\n",
            "ite = 142   error = 0.00002163\n",
            "ite = 143   error = 0.00002133\n",
            "ite = 144   error = 0.00002103\n",
            "ite = 145   error = 0.00002073\n",
            "ite = 146   error = 0.00002044\n",
            "ite = 147   error = 0.00002016\n",
            "ite = 148   error = 0.00001988\n",
            "ite = 149   error = 0.00001961\n",
            "ite = 150   error = 0.00001934\n",
            "ite = 151   error = 0.00001908\n",
            "ite = 152   error = 0.00001883\n",
            "ite = 153   error = 0.00001857\n",
            "ite = 154   error = 0.00001833\n",
            "ite = 155   error = 0.00001808\n",
            "ite = 156   error = 0.00001785\n",
            "ite = 157   error = 0.00001761\n",
            "ite = 158   error = 0.00001739\n",
            "ite = 159   error = 0.00001716\n",
            "ite = 160   error = 0.00001694\n",
            "ite = 161   error = 0.00001673\n",
            "ite = 162   error = 0.00001652\n",
            "ite = 163   error = 0.00001631\n",
            "ite = 164   error = 0.00001611\n",
            "ite = 165   error = 0.00001591\n",
            "ite = 166   error = 0.00001571\n",
            "ite = 167   error = 0.00001552\n",
            "ite = 168   error = 0.00001533\n",
            "ite = 169   error = 0.00001515\n",
            "ite = 170   error = 0.00001497\n",
            "ite = 171   error = 0.00001479\n",
            "ite = 172   error = 0.00001462\n",
            "ite = 173   error = 0.00001445\n",
            "ite = 174   error = 0.00001428\n",
            "ite = 175   error = 0.00001412\n",
            "ite = 176   error = 0.00001396\n",
            "ite = 177   error = 0.00001380\n",
            "ite = 178   error = 0.00001365\n",
            "ite = 179   error = 0.00001349\n",
            "ite = 180   error = 0.00001335\n",
            "ite = 181   error = 0.00001320\n",
            "ite = 182   error = 0.00001306\n",
            "ite = 183   error = 0.00001292\n",
            "ite = 184   error = 0.00001279\n",
            "ite = 185   error = 0.00001265\n",
            "ite = 186   error = 0.00001252\n",
            "ite = 187   error = 0.00001239\n",
            "ite = 188   error = 0.00001227\n",
            "ite = 189   error = 0.00001215\n",
            "ite = 190   error = 0.00001203\n",
            "ite = 191   error = 0.00001191\n",
            "ite = 192   error = 0.00001179\n",
            "ite = 193   error = 0.00001168\n",
            "ite = 194   error = 0.00001157\n",
            "ite = 195   error = 0.00001147\n",
            "ite = 196   error = 0.00001136\n",
            "ite = 197   error = 0.00001126\n",
            "ite = 198   error = 0.00001116\n",
            "ite = 199   error = 0.00001106\n",
            "ite = 200   error = 0.00001096\n",
            "ite = 201   error = 0.00001087\n",
            "ite = 202   error = 0.00001078\n",
            "ite = 203   error = 0.00001069\n",
            "ite = 204   error = 0.00001060\n",
            "ite = 205   error = 0.00001051\n",
            "ite = 206   error = 0.00001043\n",
            "ite = 207   error = 0.00001035\n",
            "ite = 208   error = 0.00001027\n",
            "ite = 209   error = 0.00001019\n",
            "ite = 210   error = 0.00001012\n",
            "ite = 211   error = 0.00001004\n",
            "ite = 212   error = 0.00000997\n",
            "ite = 213   error = 0.00000990\n",
            "ite = 214   error = 0.00000983\n",
            "ite = 215   error = 0.00000976\n",
            "ite = 216   error = 0.00000970\n",
            "ite = 217   error = 0.00000964\n",
            "ite = 218   error = 0.00000957\n",
            "ite = 219   error = 0.00000951\n",
            "ite = 220   error = 0.00000946\n",
            "ite = 221   error = 0.00000940\n",
            "ite = 222   error = 0.00000934\n",
            "ite = 223   error = 0.00000929\n",
            "ite = 224   error = 0.00000924\n",
            "ite = 225   error = 0.00000919\n",
            "ite = 226   error = 0.00000914\n",
            "ite = 227   error = 0.00000909\n",
            "ite = 228   error = 0.00000904\n",
            "ite = 229   error = 0.00000900\n",
            "ite = 230   error = 0.00000896\n",
            "ite = 231   error = 0.00000891\n",
            "ite = 232   error = 0.00000887\n",
            "ite = 233   error = 0.00000883\n",
            "ite = 234   error = 0.00000880\n",
            "ite = 235   error = 0.00000876\n",
            "ite = 236   error = 0.00000872\n",
            "ite = 237   error = 0.00000869\n",
            "ite = 238   error = 0.00000866\n",
            "ite = 239   error = 0.00000863\n",
            "ite = 240   error = 0.00000859\n",
            "ite = 241   error = 0.00000857\n",
            "ite = 242   error = 0.00000854\n",
            "ite = 243   error = 0.00000851\n",
            "ite = 244   error = 0.00000848\n",
            "ite = 245   error = 0.00000846\n",
            "ite = 246   error = 0.00000844\n",
            "ite = 247   error = 0.00000841\n",
            "ite = 248   error = 0.00000839\n",
            "ite = 249   error = 0.00000837\n",
            "ite = 250   error = 0.00000835\n",
            "ite = 251   error = 0.00000833\n",
            "ite = 252   error = 0.00000832\n",
            "ite = 253   error = 0.00000830\n",
            "ite = 254   error = 0.00000829\n",
            "ite = 255   error = 0.00000827\n",
            "ite = 256   error = 0.00000826\n",
            "ite = 257   error = 0.00000825\n",
            "ite = 258   error = 0.00000823\n",
            "ite = 259   error = 0.00000822\n",
            "ite = 260   error = 0.00000821\n",
            "ite = 261   error = 0.00000821\n",
            "ite = 262   error = 0.00000820\n",
            "ite = 263   error = 0.00000819\n",
            "ite = 264   error = 0.00000818\n",
            "ite = 265   error = 0.00000818\n",
            "ite = 266   error = 0.00000818\n",
            "ite = 267   error = 0.00000817\n",
            "ite = 268   error = 0.00000817\n",
            "ite = 269   error = 0.00000817\n",
            "ite = 270   error = 0.00000817\n",
            "ite = 271   error = 0.00000817\n",
            "ite = 272   error = 0.00000817\n",
            "ite = 273   error = 0.00000817\n",
            "ite = 274   error = 0.00000817\n",
            "ite = 275   error = 0.00000817\n",
            "ite = 276   error = 0.00000818\n",
            "ite = 277   error = 0.00000818\n",
            "ite = 278   error = 0.00000818\n",
            "ite = 279   error = 0.00000819\n",
            "ite = 280   error = 0.00000820\n",
            "ite = 281   error = 0.00000820\n",
            "ite = 282   error = 0.00000821\n",
            "ite = 283   error = 0.00000822\n",
            "ite = 284   error = 0.00000823\n",
            "ite = 285   error = 0.00000824\n",
            "ite = 286   error = 0.00000825\n",
            "ite = 287   error = 0.00000826\n",
            "ite = 288   error = 0.00000827\n",
            "ite = 289   error = 0.00000829\n",
            "ite = 290   error = 0.00000830\n",
            "ite = 291   error = 0.00000831\n",
            "ite = 292   error = 0.00000833\n",
            "ite = 293   error = 0.00000834\n",
            "ite = 294   error = 0.00000836\n",
            "ite = 295   error = 0.00000837\n",
            "ite = 296   error = 0.00000839\n",
            "ite = 297   error = 0.00000841\n",
            "ite = 298   error = 0.00000843\n",
            "ite = 299   error = 0.00000845\n",
            "ite = 300   error = 0.00000847\n",
            "ite = 301   error = 0.00000849\n",
            "ite = 302   error = 0.00000851\n",
            "ite = 303   error = 0.00000853\n",
            "ite = 304   error = 0.00000855\n",
            "ite = 305   error = 0.00000857\n",
            "ite = 306   error = 0.00000859\n",
            "ite = 307   error = 0.00000862\n",
            "ite = 308   error = 0.00000864\n",
            "ite = 309   error = 0.00000867\n",
            "ite = 310   error = 0.00000869\n",
            "ite = 311   error = 0.00000872\n",
            "ite = 312   error = 0.00000874\n",
            "ite = 313   error = 0.00000877\n",
            "ite = 314   error = 0.00000880\n",
            "ite = 315   error = 0.00000883\n",
            "ite = 316   error = 0.00000885\n",
            "ite = 317   error = 0.00000888\n",
            "ite = 318   error = 0.00000891\n",
            "ite = 319   error = 0.00000894\n",
            "ite = 320   error = 0.00000897\n",
            "ite = 321   error = 0.00000901\n",
            "ite = 322   error = 0.00000904\n",
            "ite = 323   error = 0.00000907\n",
            "ite = 324   error = 0.00000910\n",
            "ite = 325   error = 0.00000914\n",
            "ite = 326   error = 0.00000917\n",
            "ite = 327   error = 0.00000920\n",
            "ite = 328   error = 0.00000924\n",
            "ite = 329   error = 0.00000927\n",
            "ite = 330   error = 0.00000931\n",
            "ite = 331   error = 0.00000935\n",
            "ite = 332   error = 0.00000938\n",
            "ite = 333   error = 0.00000942\n",
            "ite = 334   error = 0.00000946\n",
            "ite = 335   error = 0.00000950\n",
            "ite = 336   error = 0.00000954\n",
            "ite = 337   error = 0.00000958\n",
            "ite = 338   error = 0.00000962\n",
            "ite = 339   error = 0.00000966\n",
            "ite = 340   error = 0.00000970\n",
            "ite = 341   error = 0.00000974\n",
            "ite = 342   error = 0.00000979\n",
            "ite = 343   error = 0.00000983\n",
            "ite = 344   error = 0.00000987\n",
            "ite = 345   error = 0.00000992\n",
            "ite = 346   error = 0.00000996\n",
            "ite = 347   error = 0.00001001\n",
            "ite = 348   error = 0.00001006\n",
            "ite = 349   error = 0.00001010\n",
            "ite = 350   error = 0.00001015\n",
            "ite = 351   error = 0.00001020\n",
            "ite = 352   error = 0.00001025\n",
            "ite = 353   error = 0.00001029\n",
            "ite = 354   error = 0.00001034\n",
            "ite = 355   error = 0.00001039\n",
            "ite = 356   error = 0.00001045\n",
            "ite = 357   error = 0.00001050\n",
            "ite = 358   error = 0.00001055\n",
            "ite = 359   error = 0.00001060\n",
            "ite = 360   error = 0.00001065\n",
            "ite = 361   error = 0.00001071\n",
            "ite = 362   error = 0.00001076\n",
            "ite = 363   error = 0.00001082\n",
            "ite = 364   error = 0.00001087\n",
            "ite = 365   error = 0.00001093\n",
            "ite = 366   error = 0.00001099\n",
            "ite = 367   error = 0.00001104\n",
            "ite = 368   error = 0.00001110\n",
            "ite = 369   error = 0.00001116\n",
            "ite = 370   error = 0.00001122\n",
            "ite = 371   error = 0.00001128\n",
            "ite = 372   error = 0.00001134\n",
            "ite = 373   error = 0.00001140\n",
            "ite = 374   error = 0.00001146\n",
            "ite = 375   error = 0.00001153\n",
            "ite = 376   error = 0.00001159\n",
            "ite = 377   error = 0.00001165\n",
            "ite = 378   error = 0.00001172\n",
            "ite = 379   error = 0.00001178\n",
            "ite = 380   error = 0.00001185\n",
            "ite = 381   error = 0.00001192\n",
            "ite = 382   error = 0.00001198\n",
            "ite = 383   error = 0.00001205\n",
            "ite = 384   error = 0.00001212\n",
            "ite = 385   error = 0.00001219\n",
            "ite = 386   error = 0.00001226\n",
            "ite = 387   error = 0.00001233\n",
            "ite = 388   error = 0.00001240\n",
            "ite = 389   error = 0.00001248\n",
            "ite = 390   error = 0.00001255\n",
            "ite = 391   error = 0.00001262\n",
            "ite = 392   error = 0.00001270\n",
            "ite = 393   error = 0.00001277\n",
            "ite = 394   error = 0.00001285\n",
            "ite = 395   error = 0.00001293\n",
            "ite = 396   error = 0.00001300\n",
            "ite = 397   error = 0.00001308\n",
            "ite = 398   error = 0.00001316\n",
            "ite = 399   error = 0.00001324\n",
            "ite = 400   error = 0.00001332\n",
            "ite = 401   error = 0.00001340\n",
            "ite = 402   error = 0.00001349\n",
            "ite = 403   error = 0.00001357\n",
            "ite = 404   error = 0.00001365\n",
            "ite = 405   error = 0.00001374\n",
            "ite = 406   error = 0.00001382\n",
            "ite = 407   error = 0.00001391\n",
            "ite = 408   error = 0.00001400\n",
            "ite = 409   error = 0.00001408\n",
            "ite = 410   error = 0.00001417\n",
            "ite = 411   error = 0.00001426\n",
            "ite = 412   error = 0.00001435\n",
            "ite = 413   error = 0.00001444\n",
            "ite = 414   error = 0.00001453\n",
            "ite = 415   error = 0.00001463\n",
            "ite = 416   error = 0.00001472\n",
            "ite = 417   error = 0.00001482\n",
            "ite = 418   error = 0.00001491\n",
            "ite = 419   error = 0.00001501\n",
            "ite = 420   error = 0.00001510\n",
            "ite = 421   error = 0.00001520\n",
            "ite = 422   error = 0.00001530\n",
            "ite = 423   error = 0.00001540\n",
            "ite = 424   error = 0.00001550\n",
            "ite = 425   error = 0.00001560\n",
            "ite = 426   error = 0.00001570\n",
            "ite = 427   error = 0.00001580\n",
            "ite = 428   error = 0.00001591\n",
            "ite = 429   error = 0.00001601\n",
            "ite = 430   error = 0.00001612\n",
            "ite = 431   error = 0.00001622\n",
            "ite = 432   error = 0.00001633\n",
            "ite = 433   error = 0.00001643\n",
            "ite = 434   error = 0.00001654\n",
            "ite = 435   error = 0.00001665\n",
            "ite = 436   error = 0.00001676\n",
            "ite = 437   error = 0.00001687\n",
            "ite = 438   error = 0.00001698\n",
            "ite = 439   error = 0.00001710\n",
            "ite = 440   error = 0.00001721\n",
            "ite = 441   error = 0.00001732\n",
            "ite = 442   error = 0.00001744\n",
            "ite = 443   error = 0.00001755\n",
            "ite = 444   error = 0.00001767\n",
            "ite = 445   error = 0.00001778\n",
            "ite = 446   error = 0.00001790\n",
            "ite = 447   error = 0.00001802\n",
            "ite = 448   error = 0.00001814\n",
            "ite = 449   error = 0.00001826\n",
            "ite = 450   error = 0.00001838\n",
            "ite = 451   error = 0.00001850\n",
            "ite = 452   error = 0.00001862\n",
            "ite = 453   error = 0.00001874\n",
            "ite = 454   error = 0.00001886\n",
            "ite = 455   error = 0.00001899\n",
            "ite = 456   error = 0.00001911\n",
            "ite = 457   error = 0.00001923\n",
            "ite = 458   error = 0.00001936\n",
            "ite = 459   error = 0.00001948\n",
            "ite = 460   error = 0.00001961\n",
            "ite = 461   error = 0.00001974\n",
            "ite = 462   error = 0.00001986\n",
            "ite = 463   error = 0.00001999\n",
            "ite = 464   error = 0.00002012\n",
            "ite = 465   error = 0.00002025\n",
            "ite = 466   error = 0.00002037\n",
            "ite = 467   error = 0.00002050\n",
            "ite = 468   error = 0.00002063\n",
            "ite = 469   error = 0.00002076\n",
            "ite = 470   error = 0.00002089\n",
            "ite = 471   error = 0.00002102\n",
            "ite = 472   error = 0.00002115\n",
            "ite = 473   error = 0.00002128\n",
            "ite = 474   error = 0.00002142\n",
            "ite = 475   error = 0.00002155\n",
            "ite = 476   error = 0.00002168\n",
            "ite = 477   error = 0.00002181\n",
            "ite = 478   error = 0.00002194\n",
            "ite = 479   error = 0.00002207\n",
            "ite = 480   error = 0.00002220\n",
            "ite = 481   error = 0.00002234\n",
            "ite = 482   error = 0.00002247\n",
            "ite = 483   error = 0.00002260\n",
            "ite = 484   error = 0.00002273\n",
            "ite = 485   error = 0.00002286\n",
            "ite = 486   error = 0.00002300\n",
            "ite = 487   error = 0.00002313\n",
            "ite = 488   error = 0.00002326\n",
            "ite = 489   error = 0.00002339\n",
            "ite = 490   error = 0.00002352\n",
            "ite = 491   error = 0.00002365\n",
            "ite = 492   error = 0.00002378\n",
            "ite = 493   error = 0.00002391\n",
            "ite = 494   error = 0.00002404\n",
            "ite = 495   error = 0.00002417\n",
            "ite = 496   error = 0.00002430\n",
            "ite = 497   error = 0.00002442\n",
            "ite = 498   error = 0.00002455\n",
            "ite = 499   error = 0.00002468\n",
            "ite = 500   error = 0.00002480\n",
            "ite = 501   error = 0.00002493\n",
            "ite = 502   error = 0.00002505\n",
            "ite = 503   error = 0.00002518\n",
            "ite = 504   error = 0.00002530\n",
            "ite = 505   error = 0.00002543\n",
            "ite = 506   error = 0.00002555\n",
            "ite = 507   error = 0.00002567\n",
            "ite = 508   error = 0.00002579\n",
            "ite = 509   error = 0.00002591\n",
            "ite = 510   error = 0.00002603\n",
            "ite = 511   error = 0.00002614\n",
            "ite = 512   error = 0.00002626\n",
            "ite = 513   error = 0.00002638\n",
            "ite = 514   error = 0.00002649\n",
            "ite = 515   error = 0.00002661\n",
            "ite = 516   error = 0.00002672\n",
            "ite = 517   error = 0.00002683\n",
            "ite = 518   error = 0.00002694\n",
            "ite = 519   error = 0.00002705\n",
            "ite = 520   error = 0.00002716\n",
            "ite = 521   error = 0.00002726\n",
            "ite = 522   error = 0.00002737\n",
            "ite = 523   error = 0.00002747\n",
            "ite = 524   error = 0.00002757\n",
            "ite = 525   error = 0.00002768\n",
            "ite = 526   error = 0.00002778\n",
            "ite = 527   error = 0.00002787\n",
            "ite = 528   error = 0.00002797\n",
            "ite = 529   error = 0.00002807\n",
            "ite = 530   error = 0.00002816\n",
            "ite = 531   error = 0.00002826\n",
            "ite = 532   error = 0.00002835\n",
            "ite = 533   error = 0.00002844\n",
            "ite = 534   error = 0.00002853\n",
            "ite = 535   error = 0.00002861\n",
            "ite = 536   error = 0.00002870\n",
            "ite = 537   error = 0.00002878\n",
            "ite = 538   error = 0.00002886\n",
            "ite = 539   error = 0.00002895\n",
            "ite = 540   error = 0.00002903\n",
            "ite = 541   error = 0.00002910\n",
            "ite = 542   error = 0.00002918\n",
            "ite = 543   error = 0.00002925\n",
            "ite = 544   error = 0.00002933\n",
            "ite = 545   error = 0.00002940\n",
            "ite = 546   error = 0.00002947\n",
            "ite = 547   error = 0.00002954\n",
            "ite = 548   error = 0.00002960\n",
            "ite = 549   error = 0.00002967\n",
            "ite = 550   error = 0.00002973\n",
            "ite = 551   error = 0.00002979\n",
            "ite = 552   error = 0.00002985\n",
            "ite = 553   error = 0.00002991\n",
            "ite = 554   error = 0.00002997\n",
            "ite = 555   error = 0.00003002\n",
            "ite = 556   error = 0.00003008\n",
            "ite = 557   error = 0.00003013\n",
            "ite = 558   error = 0.00003018\n",
            "ite = 559   error = 0.00003023\n",
            "ite = 560   error = 0.00003027\n",
            "ite = 561   error = 0.00003032\n",
            "ite = 562   error = 0.00003036\n",
            "ite = 563   error = 0.00003041\n",
            "ite = 564   error = 0.00003045\n",
            "ite = 565   error = 0.00003049\n",
            "ite = 566   error = 0.00003052\n",
            "ite = 567   error = 0.00003056\n",
            "ite = 568   error = 0.00003060\n",
            "ite = 569   error = 0.00003063\n",
            "ite = 570   error = 0.00003066\n",
            "ite = 571   error = 0.00003069\n",
            "ite = 572   error = 0.00003072\n",
            "ite = 573   error = 0.00003075\n",
            "ite = 574   error = 0.00003077\n",
            "ite = 575   error = 0.00003080\n",
            "ite = 576   error = 0.00003082\n",
            "ite = 577   error = 0.00003084\n",
            "ite = 578   error = 0.00003086\n",
            "ite = 579   error = 0.00003088\n",
            "ite = 580   error = 0.00003090\n",
            "ite = 581   error = 0.00003092\n",
            "ite = 582   error = 0.00003093\n",
            "ite = 583   error = 0.00003095\n",
            "ite = 584   error = 0.00003096\n",
            "ite = 585   error = 0.00003097\n",
            "ite = 586   error = 0.00003098\n",
            "ite = 587   error = 0.00003099\n",
            "ite = 588   error = 0.00003100\n",
            "ite = 589   error = 0.00003100\n",
            "ite = 590   error = 0.00003101\n",
            "ite = 591   error = 0.00003101\n",
            "ite = 592   error = 0.00003102\n",
            "ite = 593   error = 0.00003102\n",
            "ite = 594   error = 0.00003102\n",
            "ite = 595   error = 0.00003102\n",
            "ite = 596   error = 0.00003102\n",
            "ite = 597   error = 0.00003102\n",
            "ite = 598   error = 0.00003102\n",
            "ite = 599   error = 0.00003101\n",
            "ite = 600   error = 0.00003101\n",
            "ite = 601   error = 0.00003100\n",
            "ite = 602   error = 0.00003100\n",
            "ite = 603   error = 0.00003099\n",
            "ite = 604   error = 0.00003098\n",
            "ite = 605   error = 0.00003097\n",
            "ite = 606   error = 0.00003096\n",
            "ite = 607   error = 0.00003095\n",
            "ite = 608   error = 0.00003094\n",
            "ite = 609   error = 0.00003093\n",
            "ite = 610   error = 0.00003092\n",
            "ite = 611   error = 0.00003090\n",
            "ite = 612   error = 0.00003089\n",
            "ite = 613   error = 0.00003088\n",
            "ite = 614   error = 0.00003086\n",
            "ite = 615   error = 0.00003085\n",
            "ite = 616   error = 0.00003083\n",
            "ite = 617   error = 0.00003081\n",
            "ite = 618   error = 0.00003080\n",
            "ite = 619   error = 0.00003078\n",
            "ite = 620   error = 0.00003076\n",
            "ite = 621   error = 0.00003074\n",
            "ite = 622   error = 0.00003072\n",
            "ite = 623   error = 0.00003070\n",
            "ite = 624   error = 0.00003068\n",
            "ite = 625   error = 0.00003066\n",
            "ite = 626   error = 0.00003064\n",
            "ite = 627   error = 0.00003062\n",
            "ite = 628   error = 0.00003060\n",
            "ite = 629   error = 0.00003058\n",
            "ite = 630   error = 0.00003056\n",
            "ite = 631   error = 0.00003054\n",
            "ite = 632   error = 0.00003052\n",
            "ite = 633   error = 0.00003049\n",
            "ite = 634   error = 0.00003047\n",
            "ite = 635   error = 0.00003045\n",
            "ite = 636   error = 0.00003043\n",
            "ite = 637   error = 0.00003040\n",
            "ite = 638   error = 0.00003038\n",
            "ite = 639   error = 0.00003036\n",
            "ite = 640   error = 0.00003034\n",
            "ite = 641   error = 0.00003031\n",
            "ite = 642   error = 0.00003029\n",
            "ite = 643   error = 0.00003027\n",
            "ite = 644   error = 0.00003024\n",
            "ite = 645   error = 0.00003022\n",
            "ite = 646   error = 0.00003020\n",
            "ite = 647   error = 0.00003017\n",
            "ite = 648   error = 0.00003015\n",
            "ite = 649   error = 0.00003013\n",
            "ite = 650   error = 0.00003011\n",
            "ite = 651   error = 0.00003008\n",
            "ite = 652   error = 0.00003006\n",
            "ite = 653   error = 0.00003004\n",
            "ite = 654   error = 0.00003002\n",
            "ite = 655   error = 0.00003000\n",
            "ite = 656   error = 0.00002997\n",
            "ite = 657   error = 0.00002995\n",
            "ite = 658   error = 0.00002993\n",
            "ite = 659   error = 0.00002991\n",
            "ite = 660   error = 0.00002989\n",
            "ite = 661   error = 0.00002987\n",
            "ite = 662   error = 0.00002985\n",
            "ite = 663   error = 0.00002983\n",
            "ite = 664   error = 0.00002981\n",
            "ite = 665   error = 0.00002979\n",
            "ite = 666   error = 0.00002977\n",
            "ite = 667   error = 0.00002975\n",
            "ite = 668   error = 0.00002973\n",
            "ite = 669   error = 0.00002971\n",
            "ite = 670   error = 0.00002970\n",
            "ite = 671   error = 0.00002968\n",
            "ite = 672   error = 0.00002966\n",
            "ite = 673   error = 0.00002965\n",
            "ite = 674   error = 0.00002963\n",
            "ite = 675   error = 0.00002962\n",
            "ite = 676   error = 0.00002960\n",
            "ite = 677   error = 0.00002959\n",
            "ite = 678   error = 0.00002957\n",
            "ite = 679   error = 0.00002956\n",
            "ite = 680   error = 0.00002954\n",
            "ite = 681   error = 0.00002953\n",
            "ite = 682   error = 0.00002952\n",
            "ite = 683   error = 0.00002951\n",
            "ite = 684   error = 0.00002950\n",
            "ite = 685   error = 0.00002949\n",
            "ite = 686   error = 0.00002948\n",
            "ite = 687   error = 0.00002947\n",
            "ite = 688   error = 0.00002946\n",
            "ite = 689   error = 0.00002945\n",
            "ite = 690   error = 0.00002944\n",
            "ite = 691   error = 0.00002944\n",
            "ite = 692   error = 0.00002943\n",
            "ite = 693   error = 0.00002942\n",
            "ite = 694   error = 0.00002942\n",
            "ite = 695   error = 0.00002941\n",
            "ite = 696   error = 0.00002941\n",
            "ite = 697   error = 0.00002941\n",
            "ite = 698   error = 0.00002941\n",
            "ite = 699   error = 0.00002940\n",
            "ite = 700   error = 0.00002940\n",
            "ite = 701   error = 0.00002940\n",
            "ite = 702   error = 0.00002940\n",
            "ite = 703   error = 0.00002940\n",
            "ite = 704   error = 0.00002941\n",
            "ite = 705   error = 0.00002941\n",
            "ite = 706   error = 0.00002941\n",
            "ite = 707   error = 0.00002942\n",
            "ite = 708   error = 0.00002942\n",
            "ite = 709   error = 0.00002943\n",
            "ite = 710   error = 0.00002943\n",
            "ite = 711   error = 0.00002944\n",
            "ite = 712   error = 0.00002945\n",
            "ite = 713   error = 0.00002946\n",
            "ite = 714   error = 0.00002947\n",
            "ite = 715   error = 0.00002948\n",
            "ite = 716   error = 0.00002949\n",
            "ite = 717   error = 0.00002951\n",
            "ite = 718   error = 0.00002952\n",
            "ite = 719   error = 0.00002953\n",
            "ite = 720   error = 0.00002955\n",
            "ite = 721   error = 0.00002957\n",
            "ite = 722   error = 0.00002958\n",
            "ite = 723   error = 0.00002960\n",
            "ite = 724   error = 0.00002962\n",
            "ite = 725   error = 0.00002964\n",
            "ite = 726   error = 0.00002966\n",
            "ite = 727   error = 0.00002968\n",
            "ite = 728   error = 0.00002971\n",
            "ite = 729   error = 0.00002973\n",
            "ite = 730   error = 0.00002976\n",
            "ite = 731   error = 0.00002978\n",
            "ite = 732   error = 0.00002981\n",
            "ite = 733   error = 0.00002984\n",
            "ite = 734   error = 0.00002987\n",
            "ite = 735   error = 0.00002990\n",
            "ite = 736   error = 0.00002993\n",
            "ite = 737   error = 0.00002996\n",
            "ite = 738   error = 0.00003000\n",
            "ite = 739   error = 0.00003003\n",
            "ite = 740   error = 0.00003007\n",
            "ite = 741   error = 0.00003011\n",
            "ite = 742   error = 0.00003015\n",
            "ite = 743   error = 0.00003019\n",
            "ite = 744   error = 0.00003023\n",
            "ite = 745   error = 0.00003027\n",
            "ite = 746   error = 0.00003031\n",
            "ite = 747   error = 0.00003036\n",
            "ite = 748   error = 0.00003040\n",
            "ite = 749   error = 0.00003045\n",
            "ite = 750   error = 0.00003050\n",
            "ite = 751   error = 0.00003055\n",
            "ite = 752   error = 0.00003060\n",
            "ite = 753   error = 0.00003066\n",
            "ite = 754   error = 0.00003071\n",
            "ite = 755   error = 0.00003077\n",
            "ite = 756   error = 0.00003082\n",
            "ite = 757   error = 0.00003088\n",
            "ite = 758   error = 0.00003094\n",
            "ite = 759   error = 0.00003100\n",
            "ite = 760   error = 0.00003107\n",
            "ite = 761   error = 0.00003113\n",
            "ite = 762   error = 0.00003120\n",
            "ite = 763   error = 0.00003127\n",
            "ite = 764   error = 0.00003134\n",
            "ite = 765   error = 0.00003141\n",
            "ite = 766   error = 0.00003148\n",
            "ite = 767   error = 0.00003156\n",
            "ite = 768   error = 0.00003163\n",
            "ite = 769   error = 0.00003171\n",
            "ite = 770   error = 0.00003179\n",
            "ite = 771   error = 0.00003187\n",
            "ite = 772   error = 0.00003196\n",
            "ite = 773   error = 0.00003204\n",
            "ite = 774   error = 0.00003213\n",
            "ite = 775   error = 0.00003222\n",
            "ite = 776   error = 0.00003231\n",
            "ite = 777   error = 0.00003240\n",
            "ite = 778   error = 0.00003250\n",
            "ite = 779   error = 0.00003259\n",
            "ite = 780   error = 0.00003269\n",
            "ite = 781   error = 0.00003279\n",
            "ite = 782   error = 0.00003290\n",
            "ite = 783   error = 0.00003300\n",
            "ite = 784   error = 0.00003311\n",
            "ite = 785   error = 0.00003322\n",
            "ite = 786   error = 0.00003333\n",
            "ite = 787   error = 0.00003345\n",
            "ite = 788   error = 0.00003356\n",
            "ite = 789   error = 0.00003368\n",
            "ite = 790   error = 0.00003380\n",
            "ite = 791   error = 0.00003393\n",
            "ite = 792   error = 0.00003405\n",
            "ite = 793   error = 0.00003418\n",
            "ite = 794   error = 0.00003431\n",
            "ite = 795   error = 0.00003445\n",
            "ite = 796   error = 0.00003458\n",
            "ite = 797   error = 0.00003472\n",
            "ite = 798   error = 0.00003487\n",
            "ite = 799   error = 0.00003501\n",
            "ite = 800   error = 0.00003516\n",
            "% aciertos X_train : 0.891\n",
            "Confusion matrix:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.81      0.84        52\n",
            "           1       0.96      0.92      0.94        53\n",
            "           2       0.83      0.95      0.89        42\n",
            "\n",
            "    accuracy                           0.89       147\n",
            "   macro avg       0.89      0.89      0.89       147\n",
            "weighted avg       0.89      0.89      0.89       147\n",
            "\n",
            "Confusion matrix:\n",
            "[[42  2  8]\n",
            " [ 4 49  0]\n",
            " [ 2  0 40]]\n",
            "% aciertos X_test : 0.921\n",
            "Confusion matrix:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.89      0.86        18\n",
            "           1       1.00      0.94      0.97        17\n",
            "           2       0.93      0.93      0.93        28\n",
            "\n",
            "    accuracy                           0.92        63\n",
            "   macro avg       0.92      0.92      0.92        63\n",
            "weighted avg       0.92      0.92      0.92        63\n",
            "\n",
            "Confusion matrix:\n",
            "[[16  0  2]\n",
            " [ 1 16  0]\n",
            " [ 2  0 26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5W5X8D2Xyvqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 3**"
      ],
      "metadata": {
        "id": "RmboJDBEz_D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*El archivo Vinos.csv tiene información referida a 13 características químicas y/o visuales de varias muestras de vinos pertenecientes a 3 clases distintas.*\n",
        "\n",
        "*Utilice el 80% de los ejemplos del archivo Vinos.csv para entrenar un multiperceptrón que sea capaz que distinguir entre las 3 clases de vinos. Observe la tasa de acierto obtenida sobre el 20% restante.*"
      ],
      "metadata": {
        "id": "8p1vM1KY2NlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "df = pd.read_csv('Vinos.csv', sep=';', encoding='utf-8')\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "HRJe4JIy2XI-",
        "outputId": "12d099fa-9e40-4b8b-ed23-c1a61123aa1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Class  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
              "0        1    14.23        1.71  2.43               15.6        127   \n",
              "1        1    13.20        1.78  2.14               11.2        100   \n",
              "2        1    13.16        2.36  2.67               18.6        101   \n",
              "3        1    14.37        1.95  2.50               16.8        113   \n",
              "4        1    13.24        2.59  2.87               21.0        118   \n",
              "..     ...      ...         ...   ...                ...        ...   \n",
              "173      3    13.71        5.65  2.45               20.5         95   \n",
              "174      3    13.40        3.91  2.48               23.0        102   \n",
              "175      3    13.27        4.28  2.26               20.0        120   \n",
              "176      3    13.17        2.59  2.37               20.0        120   \n",
              "177      3    14.13        4.10  2.74               24.5         96   \n",
              "\n",
              "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
              "0             2.80        3.06                  0.28             2.29   \n",
              "1             2.65        2.76                  0.26             1.28   \n",
              "2             2.80        3.24                  0.30             2.81   \n",
              "3             3.85        3.49                  0.24             2.18   \n",
              "4             2.80        2.69                  0.39             1.82   \n",
              "..             ...         ...                   ...              ...   \n",
              "173           1.68        0.61                  0.52             1.06   \n",
              "174           1.80        0.75                  0.43             1.41   \n",
              "175           1.59        0.69                  0.43             1.35   \n",
              "176           1.65        0.68                  0.53             1.46   \n",
              "177           2.05        0.76                  0.56             1.35   \n",
              "\n",
              "     Color intensity   Hue  OD280/OD315  Proline  \n",
              "0               5.64  1.04         3.92     1065  \n",
              "1               4.38  1.05         3.40     1050  \n",
              "2               5.68  1.03         3.17     1185  \n",
              "3               7.80  0.86         3.45     1480  \n",
              "4               4.32  1.04         2.93      735  \n",
              "..               ...   ...          ...      ...  \n",
              "173             7.70  0.64         1.74      740  \n",
              "174             7.30  0.70         1.56      750  \n",
              "175            10.20  0.59         1.56      835  \n",
              "176             9.30  0.60         1.62      840  \n",
              "177             9.20  0.61         1.60      560  \n",
              "\n",
              "[178 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Class</th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malic acid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity of ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>OD280/OD315</th>\n",
              "      <th>Proline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>3</td>\n",
              "      <td>13.71</td>\n",
              "      <td>5.65</td>\n",
              "      <td>2.45</td>\n",
              "      <td>20.5</td>\n",
              "      <td>95</td>\n",
              "      <td>1.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.52</td>\n",
              "      <td>1.06</td>\n",
              "      <td>7.70</td>\n",
              "      <td>0.64</td>\n",
              "      <td>1.74</td>\n",
              "      <td>740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>3</td>\n",
              "      <td>13.40</td>\n",
              "      <td>3.91</td>\n",
              "      <td>2.48</td>\n",
              "      <td>23.0</td>\n",
              "      <td>102</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.41</td>\n",
              "      <td>7.30</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1.56</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>3</td>\n",
              "      <td>13.27</td>\n",
              "      <td>4.28</td>\n",
              "      <td>2.26</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.69</td>\n",
              "      <td>0.43</td>\n",
              "      <td>1.35</td>\n",
              "      <td>10.20</td>\n",
              "      <td>0.59</td>\n",
              "      <td>1.56</td>\n",
              "      <td>835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>3</td>\n",
              "      <td>13.17</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.37</td>\n",
              "      <td>20.0</td>\n",
              "      <td>120</td>\n",
              "      <td>1.65</td>\n",
              "      <td>0.68</td>\n",
              "      <td>0.53</td>\n",
              "      <td>1.46</td>\n",
              "      <td>9.30</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.62</td>\n",
              "      <td>840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>3</td>\n",
              "      <td>14.13</td>\n",
              "      <td>4.10</td>\n",
              "      <td>2.74</td>\n",
              "      <td>24.5</td>\n",
              "      <td>96</td>\n",
              "      <td>2.05</td>\n",
              "      <td>0.76</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.35</td>\n",
              "      <td>9.20</td>\n",
              "      <td>0.61</td>\n",
              "      <td>1.60</td>\n",
              "      <td>560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f59a7d74-a3e3-49c3-b7b6-a4a04edfe066');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c69085e5-d0c1-45f1-9df0-2c0cfeed1e19 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divido datos del df\n",
        "# random_state hace que se obtenga el mismo subconjunto de entrenamiento\n",
        "# cada vez que se ejecute el código con el mismo valor de random_state\n",
        "X_train = df.sample(frac = 0.8, random_state = 0) # contiene 80% de los datos\n",
        "x_test = df.drop(X_train.index) # contiene el 20% restante de los datos\n",
        "\n",
        "# rtas esperadas: clases\n",
        "Y = np.array(X_train.iloc[:,0]) # selecciona filas de primera columna del df --> clase\n",
        "\n",
        "# valores de entradas\n",
        "X = X_train.iloc[:, 1:] # contiene de la columa 1 todos los datos\n",
        "X = np.array(X) # los pone en un array\n",
        "\n",
        "\n",
        "# valores con los que testeo\n",
        "X_test = np.array(x_test.iloc[:, 1:])\n",
        "Y_test = np.array(x_test.iloc[:, 0])\n",
        "\n",
        "\n",
        "# normalizo entrada\n",
        "normalizarEntrada = 1\n",
        "if normalizarEntrada:\n",
        "  # Escala valores entre 0 y 1\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "  # X_train = min_max_scaler.fit_transform(X_train)\n",
        "   # X_test = min_max_scaler.transform(X_test)\n",
        "\n",
        "print(\"Cant datos de entrenamiento: \", len(X))\n",
        "print(\"cant de datos testeo: \", len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftmg39ar31Nt",
        "outputId": "b50d1a67-9a99-45be-c610-fc2ca0b32303"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cant datos de entrenamiento:  142\n",
            "cant de datos testeo:  36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entradas = X.shape[1] # num de columnas del arreglo --> 13\n",
        "#print (entradas)\n",
        "ocultas = 12\n",
        "salidas = Y.shape[0]  # num de elementos en la dimension 0 --> 142\n",
        "#print(salidas)\n",
        "\n",
        "alfa = 0.001\n",
        "MAX_ITE = 1000\n",
        "\n",
        "FunH = 'identity'\n",
        "\n",
        "# Creación de modelo\n",
        "modelo = MLPClassifier(max_iter=MAX_ITE, hidden_layer_sizes=ocultas, alpha=alfa,\n",
        "                       solver='sgd', activation=FunH, tol=0.001,\n",
        "                       verbose=False)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "modelo.out_activation_ = 'softmax'\n",
        "\n",
        "# Medición del entrenamiento\n",
        "Y_pred = modelo.predict(X_test)\n",
        "score = modelo.score(X_test, Y_test)\n",
        "\n",
        "# imprimo rtas\n",
        "print(Y_pred)\n",
        "print(Y_test)\n",
        "\n",
        "\n",
        "# calculo manual del accuracy\n",
        "print('Efectividad: %6.2f%%' % (100*(Y_pred == Y_test).sum()/len(Y_test)) )\n",
        "print('      Score: %6.2f%%' % (score) )\n",
        "\n",
        "\n",
        "# Y_pred == Y_test --> devuelve true si la predic es correcta\n",
        "# .sum --> suma el total de true de la operación anterior"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wurNcEeS4JQ5",
        "outputId": "50d8ecf8-c630-4904-d332-81106b18b8c5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "142\n",
            "[1 1 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 3 3 3 3 2 2 2 3 3 3 3]\n",
            "[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3]\n",
            "Efectividad:  83.33%\n",
            "      Score:   0.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 4**"
      ],
      "metadata": {
        "id": "0m-Y6S_vmltI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*El archivo Balance.csv tiene información sobre un experimento psicológico realizado para evaluar el aprendizaje en los niños.*\n",
        "\n",
        "*Cada fila de la tabla tiene las características de una balanza, referidas a la longitud de los brazos izquierdo y derecho de la balanza y al peso que hay en cada brazo, y un atributo que indica si la balanza se inclina al lado izquierdo (L), derecho (R), o está balanceada (B).*\n",
        "\n",
        "*Utilice una parte de los ejemplos para entrenar un multiperceptrón que sea capaz que predecir si la balanza está inclinada a derecha, a izquierda o si está balanceada. Analice la precisión de la red sobre los ejemplos\n",
        "de entrenamiento y sobre los de testeo.*"
      ],
      "metadata": {
        "id": "mN7nMjGcm9oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#df = pd.read_csv('Balance.csv', sep=';', encoding='utf-8' )\n",
        "#df['Balance'].hist(bins=3) # accede a la columna 'Balance'\n",
        "# y con hist(bins=3) genera histograma de los valores de la colum 'Balance'\n",
        "# bins = 3 --> indica que los valores en la colum se dividirán en tres intervalos\n",
        "\n",
        "\n",
        "#-- detectando la codificación de caracteres usada ----\n",
        "with open('Balance.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())  # or readline if the file is large\n",
        "\n",
        "# recupera el archivo en un objeto dataframe de pandas utilizando la codificación detectada\n",
        "# %% Carga de dataset con formato R, G, B, Color\n",
        "df = pd.read_csv('Balance.csv', encoding=result['encoding'])\n",
        "\n",
        "# Para hacer una inspección rápida de los datos\n",
        "#print(df.head())\n",
        "#print(df.describe())\n",
        "df['Balance'].hist(bins=3)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "PRqs0EpM274v",
        "outputId": "ebe7ffba-a823-4d88-c38c-d7c2693a6e65"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Balance  LeftWeight  LeftDist  RightWeight  RightDist\n",
            "0         B           1         1            1          1\n",
            "1         R           1         1            1          2\n",
            "2         R           1         1            1          3\n",
            "3         R           1         1            1          4\n",
            "4         R           1         1            1          5\n",
            "..      ...         ...       ...          ...        ...\n",
            "620       L           5         5            5          1\n",
            "621       L           5         5            5          2\n",
            "622       L           5         5            5          3\n",
            "623       L           5         5            5          4\n",
            "624       B           5         5            5          5\n",
            "\n",
            "[625 rows x 5 columns]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# otra forma de hacerlo comparado al Ejercicio 3\n",
        "\n",
        "# %% separa atributos y clases\n",
        "X_raw = np.array(df.iloc[:,1:])  # recupera todas las columnas salvo la primera (clase)\n",
        "#X_raw = np.array(df.iloc[:,0:-1])\n",
        "Y_raw = np.array(df.iloc[:,-1])    # recupera solo la última columna (es la clase)\n",
        "print(\"X_raw\", X_raw)\n",
        "print(\"---------------------------------------\")\n",
        "# Binarizador para convertir el nombre de la clase en one hot encoding\n",
        "binarizer = preprocessing.LabelBinarizer()\n",
        "\n",
        "# Binariza cada clase como una combinación de un 1 y 0s\n",
        "Y_raw = binarizer.fit_transform(Y_raw)\n",
        "Y_binario = binarizer.fit_transform(Y_raw)\n",
        "\n",
        "print('Las clases del dataset son :', binarizer.classes_)\n",
        "#print(Y_raw)\n",
        "# Escala los atributos de los ejemplo\n",
        "scaler = preprocessing.StandardScaler()\n",
        "#scaler = preprocessing.MinMaxScaler()\n",
        "X_raw  = scaler.fit_transform( X_raw )\n",
        "\n",
        "# %% Separa ejemplos para enternamiento y testeo\n",
        "TEST_SIZE = 0.3 # proporcion entre testeo entre entrenamiento y testeo\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X_raw, Y_raw, test_size=TEST_SIZE)#, random_state=42)\n",
        "\n",
        "print('\\nDatos de Entrenamiento: %d   Datos de Testeo: %d' % (len(Y_train), len(Y_test) ))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqOF_aQSpix1",
        "outputId": "36c72af0-141a-4fde-f0ba-f0d3bffc71a1"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_raw [[1 1 1 1]\n",
            " [1 1 1 2]\n",
            " [1 1 1 3]\n",
            " ...\n",
            " [5 5 5 3]\n",
            " [5 5 5 4]\n",
            " [5 5 5 5]]\n",
            "---------------------------------------\n",
            "Las clases del dataset son : [0 1 2 3 4]\n",
            "\n",
            "Datos de Entrenamiento: 437   Datos de Testeo: 188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#entradas = X.shape[1] # num de columnas del arreglo --> 13\n",
        "#print (entradas)\n",
        "ocultas = 8\n",
        "#salidas = Y.shape[0]  # num de elementos en la dimension 0 --> 142\n",
        "#print(salidas)\n",
        "\n",
        "alfa = 0.001\n",
        "MAX_ITE = 1000\n",
        "\n",
        "FunH = 'tanh'\n",
        "\n",
        "# Creación de modelo\n",
        "modelo = MLPClassifier(max_iter=MAX_ITE, hidden_layer_sizes=ocultas, alpha=alfa,\n",
        "                       solver='sgd', activation=FunH, tol=0.001,\n",
        "                       verbose=False)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "modelo.out_activation_ = 'softmax'\n",
        "\n",
        "#  ########### Medición del testeo ######################\n",
        "Y_pred = modelo.predict(X_test)\n",
        "score = modelo.score(X_test, Y_raw)\n",
        "\n",
        "# \"invierte\" la transformacion b\n",
        "# inaria para obtener los nombres de las clases\n",
        "Y_it = binarizer.inverse_transform(Y_test)\n",
        "Y_pred_it = binarizer.inverse_transform(Y_pred)\n",
        "\n",
        "# calculo manual del accuracy\n",
        "print('Efectividad: %6.2f%%' % (100*(Y_pred_it == Y_it).sum()/len(Y_it)) )\n",
        "print('      Score: %6.2f%%' % (score) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rV7xf9Z3oQII",
        "outputId": "7c822ee5-75c2-4035-f663-21338ff0409d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-b197636e5540>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#  ########### Medición del testeo ######################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# \"invierte\" la transformacion b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \"\"\"\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y_pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [625, 188]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiAWnYSDswD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 5**"
      ],
      "metadata": {
        "id": "50hSxuYMwelg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*El archivo ZOO.csv contiene información de 101 animales caracterizados por los siguientes atributos*\n",
        "\n",
        "*1.Nombre del animal 2. Tiene Pelo 3. Plumas 4. Huevos 5. Leche\n",
        "Vuela 7. Acuático 8. Depredador 9. Dentado 10. Vertebrado 11. Branquias 12. Venenoso 13. Aletas 14. Patas 15. Cola 16. Domestico 17. Tamaño gato 18. Clase*\n",
        "\n",
        "\n",
        "*Salvo los atributos 1 y 18 que contienen texto y el 14 que contiene el número de patas del animal, el resto\n",
        "toma el valor 1 si el animal posee la característica y 0 si no. Hay 7 valores de clase posible (atributo 18):\n",
        "mamífero, ave, pez, invertebrado, insecto, reptil y anfibio.*\n",
        "\n",
        "\n",
        "\n",
        "*Entrene un multiperceptrón que sea capaz de clasificar un animal en una de las 7 clases. Utilice el 70% de\n",
        "los ejemplos para entrenar y el 30% para realizar el testeo. Realice al menos 10 ejecuciones\n",
        "independientes de la configuración seleccionada para respaldar sus afirmaciones referidas a la\n",
        "performance del modelo.*"
      ],
      "metadata": {
        "id": "lCgP8LlHwoWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "GOnWk2hYwmx9"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "with open('zoo.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "\n",
        "encoding = result['encoding']\n",
        "print(f\"Codificación detectada: {encoding}\")\n",
        "\n",
        "df = pd.read_csv('zoo.csv', encoding=encoding)\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "d1qF0hHlyrWW",
        "outputId": "ad1b67e5-5664-4728-fbbe-028b237b1939"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Codificación detectada: ISO-8859-1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             animal  Tiene_Pelo  Tiene_Plumas  Nace_de_huevo  Toma_Leche  \\\n",
              "0    oso_hormiguero           1             0              0           1   \n",
              "1          antilope           1             0              0           1   \n",
              "2            robalo           0             0              1           0   \n",
              "3               oso           1             0              0           1   \n",
              "4            jabali           1             0              0           1   \n",
              "..              ...         ...           ...            ...         ...   \n",
              "96          canguro           1             0              0           1   \n",
              "97           avispa           1             0              1           0   \n",
              "98             lobo           1             0              0           1   \n",
              "99           gusano           0             0              1           0   \n",
              "100      troglodito           0             1              1           0   \n",
              "\n",
              "     Vuela  Acuatico  Depredador  Dentado  Vertebrado  Respira  Venenoso  \\\n",
              "0        0         0           1        1           1        1         0   \n",
              "1        0         0           0        1           1        1         0   \n",
              "2        0         1           1        1           1        0         0   \n",
              "3        0         0           1        1           1        1         0   \n",
              "4        0         0           1        1           1        1         0   \n",
              "..     ...       ...         ...      ...         ...      ...       ...   \n",
              "96       0         0           0        1           1        1         0   \n",
              "97       1         0           0        0           0        1         1   \n",
              "98       0         0           1        1           1        1         0   \n",
              "99       0         0           0        0           0        1         0   \n",
              "100      1         0           0        0           1        1         0   \n",
              "\n",
              "     Tiene_Aletas  Cant_Patas  Tiene_Cola  Domestico  Tamano_Gato  \\\n",
              "0               0           4           0          0            1   \n",
              "1               0           4           1          0            1   \n",
              "2               1           0           1          0            0   \n",
              "3               0           4           0          0            1   \n",
              "4               0           4           1          0            1   \n",
              "..            ...         ...         ...        ...          ...   \n",
              "96              0           2           1          0            1   \n",
              "97              0           6           0          0            0   \n",
              "98              0           4           1          0            1   \n",
              "99              0           0           0          0            0   \n",
              "100             0           2           1          0            0   \n",
              "\n",
              "            Clase  \n",
              "0        Mamifero  \n",
              "1        Mamifero  \n",
              "2             Pez  \n",
              "3        Mamifero  \n",
              "4        Mamifero  \n",
              "..            ...  \n",
              "96       Mamifero  \n",
              "97        Insecto  \n",
              "98       Mamifero  \n",
              "99   Invertebrado  \n",
              "100           Ave  \n",
              "\n",
              "[101 rows x 18 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a49a29b-74c8-40b8-a75a-a48c29393650\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>animal</th>\n",
              "      <th>Tiene_Pelo</th>\n",
              "      <th>Tiene_Plumas</th>\n",
              "      <th>Nace_de_huevo</th>\n",
              "      <th>Toma_Leche</th>\n",
              "      <th>Vuela</th>\n",
              "      <th>Acuatico</th>\n",
              "      <th>Depredador</th>\n",
              "      <th>Dentado</th>\n",
              "      <th>Vertebrado</th>\n",
              "      <th>Respira</th>\n",
              "      <th>Venenoso</th>\n",
              "      <th>Tiene_Aletas</th>\n",
              "      <th>Cant_Patas</th>\n",
              "      <th>Tiene_Cola</th>\n",
              "      <th>Domestico</th>\n",
              "      <th>Tamano_Gato</th>\n",
              "      <th>Clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>oso_hormiguero</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>antilope</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>robalo</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Pez</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>oso</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jabali</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>canguro</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>avispa</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Insecto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>lobo</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Mamifero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>gusano</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Invertebrado</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>troglodito</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Ave</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>101 rows × 18 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a49a29b-74c8-40b8-a75a-a48c29393650')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a49a29b-74c8-40b8-a75a-a48c29393650 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a49a29b-74c8-40b8-a75a-a48c29393650');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3003dd0-9ed6-46a7-91dc-64d72508f547\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3003dd0-9ed6-46a7-91dc-64d72508f547')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3003dd0-9ed6-46a7-91dc-64d72508f547 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# divido datos del df\n",
        "X_train = df.sample(frac = 0.7, random_state = 1) # contiene 70% de los datos\n",
        "x_test = df.drop(X_train.index) # contiene el 30% restante de los datos\n",
        "\n",
        "# valores de entradas\n",
        "X = X_train.iloc[:, 1:-1] # entradas --> todas las colum menos la ult (clase)\n",
        "X = np.array(X) # los pone en un array\n",
        "\n",
        "# rtas esperadas\n",
        "Y = np.array(X_train.iloc[:,-1]) # salida --> ult columna (clase)\n",
        "\n",
        "# valores con los que testeo\n",
        "X_test = np.array(x_test.iloc[:, 1:-1]) # valores de entrada con los q vamos a testear\n",
        "Y_test = np.array(x_test.iloc[:, -1]) # valores de salida con los que vamos a testear\n",
        "\n",
        "# BINARIZO LAS SALIDAS --> devuelve 1 unicamente para la clase correspondiente\n",
        "binarizer = preprocessing.LabelBinarizer()\n",
        "\n",
        "Y_bin = binarizer.fit_transform(Y)\n",
        "Y_test_bin = binarizer.fit_transform(Y_test)\n",
        "\n",
        "print(\"Clases del dataset: \", binarizer.classes_)\n",
        "print('\\nDatos de Entrenamiento: %d   Datos de Testeo: %d' % (len(Y_bin), len(Y_test_bin) ))\n",
        "\n",
        "\n",
        "\n",
        "# normalizo entrada\n",
        "normalizarEntrada = 1\n",
        "if normalizarEntrada:\n",
        "  # Escala valores entre 0 y 1\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "print(\"Cant datos de entrenamiento: \", len(X))\n",
        "print(\"cant de datos testeo: \", len(X_test))\n",
        "\n",
        "df['Clase'].hist(bins = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "4nOEnMUZz6dn",
        "outputId": "ffbd9156-ed40-4b7e-c790-b7f38230c22e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases del dataset:  ['Anfibio' 'Ave' 'Insecto' 'Invertebrado' 'Mamifero' 'Pez' 'Reptil']\n",
            "\n",
            "Datos de Entrenamiento: 71   Datos de Testeo: 30\n",
            "Cant datos de entrenamiento:  71\n",
            "cant de datos testeo:  30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entradas = X.shape[1] # num de columnas del arreglo --> 13\n",
        "#print (entradas)\n",
        "ocultas = 10\n",
        "salidas = Y.shape[0]  # num de elementos en la dimension 0 --> 142\n",
        "#print(salidas)\n",
        "\n",
        "alfa = 0.001\n",
        "MAX_ITE = 1000\n",
        "\n",
        "FunH = 'identity'\n",
        "\n",
        "# Creación de modelo\n",
        "modelo = MLPClassifier(max_iter=MAX_ITE, hidden_layer_sizes=ocultas, alpha=alfa,\n",
        "                       solver='sgd', activation=FunH, tol=0.001,\n",
        "                       verbose=False)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "modelo.out_activation_ = 'softmax'\n",
        "\n",
        "# Medición del entrenamiento\n",
        "Y_pred = modelo.predict(X_test)\n",
        "score = modelo.score(X_test, Y_test)\n",
        "\n",
        "# imprimo rtas\n",
        "#print(Y_pred)\n",
        "#print(Y_test)\n",
        "\n",
        "\n",
        "# para calcular efect, score, matriz confusión y no obtener formato binario\n",
        "# hacemos la transformación inversa\n",
        "#Y_pred_it = binarizer.inverse_transform(Y_pred)\n",
        "Y_test_it = binarizer.inverse_transform(Y_test_bin)\n",
        "\n",
        "# calculo manual del accuracy\n",
        "print('Efectividad: %6.2f%%' % (100*(Y_pred == Y_test_it).sum()/len(Y_test_it)) )\n",
        "print('      Score: %6.2f%%' % (score) )\n",
        "\n",
        "\n",
        "# Y_pred == Y_test --> devuelve true si la predic es correcta\n",
        "# .sum --> suma el total de true de la operación anterior"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0EfX35R7E1r",
        "outputId": "34ba5e85-717d-443a-8470-54a6de7062ae"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Efectividad:  90.00%\n",
            "      Score:   0.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 6**"
      ],
      "metadata": {
        "id": "v4BgKNK9-6Rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Los archivos Segment_Train.csv y Segment_Test.csv contienen información referida a regiones de 3x3 pixeles pertenecientes a 7 imágenes distintas. Cada una corresponde a uno de los siguientes tipos de superficie: ladrillo, cielo, follaje, cemento, ventana, camino y pasto.*\n",
        "\n",
        "*Cada región de 3x3 ha sido caracterizada por 19 atributos numéricos:*\n",
        "1. region-centroid-col: la columna del pixel central de la región.\n",
        "2. region-centroid-row: la fila del pixel central de la región.\n",
        "3. region-pixel-count: el número de pixeles de la región = 9.\n",
        "4. short-line-density-5: el resultado de un algoritmo de extracción de líneas que cuenta la\n",
        "cantidad de líneas de bajo contraste que atraviesan la región.\n",
        "5. short-line-density-2: ídem anterior para líneas de alto contraste.\n",
        "6. vedge-mean: medida del contraste entre pixeles adyacentes. Este atributo contiene el valor\n",
        "promedio y el siguiente la desviación. Estas medidas sirven para detectar la presencia de un eje\n",
        "vertical.\n",
        "7. vegde-sd: (ver 6)\n",
        "8. hedge-mean: ídem 6 para eje horizontal. Contiene el valor medio y el siguiente la desviación.\n",
        "9. hedge-sd: (ver 8).\n",
        "10. intensity-mean: El promedio calculado sobre la región de la forma (R + G + B)/3\n",
        "11. rawred-mean: el promedio sobre la región de los valores R.\n",
        "12. rawblue-mean: el promedio sobre la región de los valores B.\n",
        "13. rawgreen-mean: el promedio sobre la región de los valores G.\n",
        "14. exred-mean: Medida de exceso de color rojo: (2R - (G + B))\n",
        "15. exblue-mean: Medida de exceso de color azul: (2B - (G + R))\n",
        "16. exgreen-mean: Medida de exceso de color verde: (2G - (R + B))\n",
        "17. value-mean: Transformación no lineal 3D de RGB.\n",
        "18. saturatoin-mean: (ver 17)\n",
        "19. hue-mean: ver 17\n",
        "\n",
        "*El atributo 20 corresponde al número de imagen de la cual fue extraída la región de 3x3. Sus valores son:\n",
        "1 (ladrillo), 2 (cemento), 3(follaje), 4 (pasto), 5 (camino), 6 (cielo), 7 (ventana).*\n",
        "\n",
        "*Entrene una red neuronal multiperceptrón para que dada una región de 3x3, representada a través de los 19 atributos indicados anteriormente, sea capaz de identificar a cuál de las 7 imágenes corresponde.*\n",
        "\n",
        "*Utilice los ejemplos del archivo Segment_Train.csv para entrenar y los del archivo Segment_Test.csv para realizar el testeo.*\n",
        "\n",
        "\n",
        "*Realice al menos 10 ejecuciones independientes de la configuración seleccionada para respaldar sus afirmaciones referidas a la performance del modelo.*"
      ],
      "metadata": {
        "id": "tkT6V_Hr_Dgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "-ovlSlnhAjpn"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "with open('Segment_Train.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "\n",
        "encoding = result['encoding']\n",
        "print(f\"Codificación detectada: {encoding}\")\n",
        "\n",
        "df_train = pd.read_csv('Segment_Train.csv', encoding=encoding)\n",
        "print(\"df-train: \")\n",
        "print(df_train)\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "with open('Segment_Test.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "\n",
        "encoding = result['encoding']\n",
        "print(f\"Codificación detectada: {encoding}\")\n",
        "\n",
        "df_test = pd.read_csv('Segment_Test.csv', encoding=encoding)\n",
        "print(\"df_test: \")\n",
        "print(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVOuwXvmApBS",
        "outputId": "7415fc27-cef4-40de-8c5a-1e818056c43d"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Codificación detectada: ascii\n",
            "df-train: \n",
            "        REGION-CENTROID-COL  REGION-CENTROID-ROW  REGION-PIXEL-COUNT  \\\n",
            "GRASS                 110.0                189.0                   9   \n",
            "GRASS                  86.0                187.0                   9   \n",
            "GRASS                 225.0                244.0                   9   \n",
            "GRASS                  47.0                232.0                   9   \n",
            "GRASS                  97.0                186.0                   9   \n",
            "...                     ...                  ...                 ...   \n",
            "CEMENT                 32.0                158.0                   9   \n",
            "CEMENT                  8.0                162.0                   9   \n",
            "CEMENT                128.0                161.0                   9   \n",
            "CEMENT                150.0                158.0                   9   \n",
            "CEMENT                124.0                162.0                   9   \n",
            "\n",
            "        SHORT-LINE-DENSITY-5  SHORT-LINE-DENSITY-2  VEDGE-MEAN  VEDGE-SD  \\\n",
            "GRASS               0.000000                   0.0    1.000000  0.666667   \n",
            "GRASS               0.000000                   0.0    1.111111  0.720082   \n",
            "GRASS               0.000000                   0.0    3.388889  2.195113   \n",
            "GRASS               0.000000                   0.0    1.277778  1.254621   \n",
            "GRASS               0.000000                   0.0    1.166667  0.691215   \n",
            "...                      ...                   ...         ...       ...   \n",
            "CEMENT              0.000000                   0.0    0.944445  0.862963   \n",
            "CEMENT              0.111111                   0.0    1.611111  2.062962   \n",
            "CEMENT              0.000000                   0.0    0.555555  0.251852   \n",
            "CEMENT              0.000000                   0.0    2.166667  1.633334   \n",
            "CEMENT              0.111111                   0.0    1.388889  1.129630   \n",
            "\n",
            "        HEDGE-MEAN  HEDGE-SD  INTENSITY-MEAN  RAWRED-MEAN  RAWBLUE-MEAN  \\\n",
            "GRASS     1.222222  1.186342       12.925926    10.888889      9.222222   \n",
            "GRASS     1.444444  0.750309       13.740741    11.666667     10.333334   \n",
            "GRASS     3.000000  1.520234       12.259259    10.333334      9.333334   \n",
            "GRASS     1.000000  0.894427       12.703704    11.000000      9.000000   \n",
            "GRASS     1.166667  1.005540       15.592592    13.888889     11.777778   \n",
            "...            ...       ...             ...          ...           ...   \n",
            "CEMENT    0.833333  0.611111        7.962963     6.333334     11.888889   \n",
            "CEMENT    0.333333  0.133333        8.370370     6.666666     12.000000   \n",
            "CEMENT    0.777778  0.162963        7.148148     5.555555     10.888889   \n",
            "CEMENT    1.388889  0.418518        8.444445     7.000000     12.222222   \n",
            "CEMENT    2.000000  0.888889       10.037037     8.000000     14.555555   \n",
            "\n",
            "        RAWGREEN-MEAN  EXRED-MEAN  EXBLUE-MEAN  EXGREEN-MEAN  VALUE-MEAN  \\\n",
            "GRASS       18.666668   -6.111111   -11.111111     17.222221   18.666668   \n",
            "GRASS       19.222221   -6.222222   -10.222222     16.444445   19.222221   \n",
            "GRASS       17.111110   -5.777778    -8.777778     14.555555   17.111110   \n",
            "GRASS       18.111110   -5.111111   -11.111111     16.222221   18.111110   \n",
            "GRASS       21.111110   -5.111111   -11.444445     16.555555   21.111110   \n",
            "...               ...         ...          ...           ...         ...   \n",
            "CEMENT       5.666666   -4.888889    11.777778     -6.888889   11.888889   \n",
            "CEMENT       6.444445   -5.111111    10.888889     -5.777778   12.000000   \n",
            "CEMENT       5.000000   -4.777778    11.222222     -6.444445   10.888889   \n",
            "CEMENT       6.111111   -4.333334    11.333333     -7.000000   12.222222   \n",
            "CEMENT       7.555555   -6.111111    13.555555     -7.444445   14.555555   \n",
            "\n",
            "        SATURATION-MEAN  HUE-MEAN  \n",
            "GRASS          0.508139  1.910864  \n",
            "GRASS          0.463329  1.941465  \n",
            "GRASS          0.480149  1.987902  \n",
            "GRASS          0.500966  1.875362  \n",
            "GRASS          0.442661  1.863654  \n",
            "...                 ...       ...  \n",
            "CEMENT         0.520578 -1.982834  \n",
            "CEMENT         0.484805 -2.044946  \n",
            "CEMENT         0.540918 -1.996307  \n",
            "CEMENT         0.503086 -1.943449  \n",
            "CEMENT         0.479931 -2.029312  \n",
            "\n",
            "[2100 rows x 19 columns]\n",
            "Codificación detectada: ascii\n",
            "df_test: \n",
            "           REGION-CENTROID-COL  REGION-CENTROID-ROW  REGION-PIXEL-COUNT  \\\n",
            "BRICKFACE                140.0                125.0                   9   \n",
            "BRICKFACE                188.0                133.0                   9   \n",
            "BRICKFACE                105.0                139.0                   9   \n",
            "BRICKFACE                 34.0                137.0                   9   \n",
            "BRICKFACE                 39.0                111.0                   9   \n",
            "...                        ...                  ...                 ...   \n",
            "GRASS                     36.0                243.0                   9   \n",
            "GRASS                    186.0                218.0                   9   \n",
            "GRASS                    197.0                236.0                   9   \n",
            "GRASS                    208.0                240.0                   9   \n",
            "GRASS                    223.0                185.0                   9   \n",
            "\n",
            "           SHORT-LINE-DENSITY-5  SHORT-LINE-DENSITY-2  VEDGE-MEAN  VEDGE-SD  \\\n",
            "BRICKFACE              0.000000                   0.0    0.277778  0.062963   \n",
            "BRICKFACE              0.000000                   0.0    0.333333  0.266667   \n",
            "BRICKFACE              0.000000                   0.0    0.277778  0.107407   \n",
            "BRICKFACE              0.000000                   0.0    0.500000  0.166667   \n",
            "BRICKFACE              0.000000                   0.0    0.722222  0.374074   \n",
            "...                         ...                   ...         ...       ...   \n",
            "GRASS                  0.111111                   0.0    1.888889  1.851851   \n",
            "GRASS                  0.000000                   0.0    1.166667  0.744444   \n",
            "GRASS                  0.000000                   0.0    2.444444  6.829628   \n",
            "GRASS                  0.111111                   0.0    1.055556  0.862963   \n",
            "GRASS                  0.000000                   0.0    0.500000  0.349603   \n",
            "\n",
            "           HEDGE-MEAN  HEDGE-SD  INTENSITY-MEAN  RAWRED-MEAN  RAWBLUE-MEAN  \\\n",
            "BRICKFACE    0.666667  0.311111        6.185185     7.333334      7.666666   \n",
            "BRICKFACE    0.500000  0.077778        6.666666     8.333334      7.777778   \n",
            "BRICKFACE    0.833333  0.522222        6.111111     7.555555      7.222222   \n",
            "BRICKFACE    1.111111  0.474074        5.851852     7.777778      6.444445   \n",
            "BRICKFACE    0.888889  0.429629        6.037037     7.000000      7.666666   \n",
            "...               ...       ...             ...          ...           ...   \n",
            "GRASS        2.000000  0.711110       13.333333     9.888889     12.111111   \n",
            "GRASS        1.166667  0.655555       13.703704    10.666667     12.666667   \n",
            "GRASS        3.333333  7.599998       16.074074    13.111111     16.666668   \n",
            "GRASS        2.444444  5.007407       14.148149    10.888889     13.000000   \n",
            "GRASS        2.388889  2.080776       12.962963    11.555555      9.777778   \n",
            "\n",
            "           RAWGREEN-MEAN  EXRED-MEAN  EXBLUE-MEAN  EXGREEN-MEAN  VALUE-MEAN  \\\n",
            "BRICKFACE       3.555556    3.444444     4.444445     -7.888889    7.777778   \n",
            "BRICKFACE       3.888889    5.000000     3.333333     -8.333333    8.444445   \n",
            "BRICKFACE       3.555556    4.333334     3.333333     -7.666666    7.555555   \n",
            "BRICKFACE       3.333333    5.777778     1.777778     -7.555555    7.777778   \n",
            "BRICKFACE       3.444444    2.888889     4.888889     -7.777778    7.888889   \n",
            "...                  ...         ...          ...           ...         ...   \n",
            "GRASS          18.000000  -10.333333    -3.666667     14.000000   18.000000   \n",
            "GRASS          17.777779   -9.111111    -3.111111     12.222222   17.777779   \n",
            "GRASS          18.444445   -8.888889     1.777778      7.111111   18.555555   \n",
            "GRASS          18.555555   -9.777778    -3.444444     13.222222   18.555555   \n",
            "GRASS          17.555555   -4.222222    -9.555555     13.777778   17.555555   \n",
            "\n",
            "           SATURATION-MEAN  HUE-MEAN  \n",
            "BRICKFACE         0.545635 -1.121818  \n",
            "BRICKFACE         0.538580 -0.924817  \n",
            "BRICKFACE         0.532628 -0.965946  \n",
            "BRICKFACE         0.573633 -0.744272  \n",
            "BRICKFACE         0.562919 -1.175773  \n",
            "...                    ...       ...  \n",
            "GRASS             0.452229  2.368311  \n",
            "GRASS             0.401347  2.382684  \n",
            "GRASS             0.292729  2.789800  \n",
            "GRASS             0.421621  2.392487  \n",
            "GRASS             0.445418  1.838850  \n",
            "\n",
            "[210 rows x 19 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# valores de entradas\n",
        "X = df_train.iloc[:, 1:] # entradas --> todas menos la clase\n",
        "X = np.array(X) # los pone en un array\n",
        "#print(X)\n",
        "\n",
        "# rtas esperadas\n",
        "Y = np.array(df_train.iloc[:,0]) # salida --> columna de clases\n",
        "\n",
        "# valores con los que testeo\n",
        "X_test = np.array(df_test.iloc[:, 1:]) # valores de entrada con los q vamos a testear\n",
        "Y_test = np.array(df_test.iloc[:, 0]) # valores de salida con los que vamos a testear\n",
        "\n",
        "# BINARIZO LAS SALIDAS --> devuelve 1 unicamente para la clase correspondiente\n",
        "binarizer = preprocessing.LabelBinarizer()\n",
        "\n",
        "Y_bin = binarizer.fit_transform(Y)\n",
        "Y_test_bin = binarizer.fit_transform(Y_test)\n",
        "\n",
        "print(\"Clases del dataset: \", binarizer.classes_)\n",
        "print('\\nDatos de Entrenamiento: %d   Datos de Testeo: %d' % (len(Y_bin), len(Y_test_bin) ))\n",
        "\n",
        "\n",
        "\n",
        "# normalizo entrada\n",
        "normalizarEntrada = 1\n",
        "if normalizarEntrada:\n",
        "  # Escala valores entre 0 y 1\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  X = min_max_scaler.fit_transform(X)\n",
        "  X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "#print(\"Cant datos de entrenamiento: \", len(X))\n",
        "#print(\"Cant de datos testeo: \", len(X_test))\n",
        "\n",
        "df['Clase'].hist(bins = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "L5FOdJreBPGC",
        "outputId": "1507493c-65e2-4a06-89fb-17c296553152"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases del dataset:  [  1.   2.   4.   5.   6.   7.   8.   9.  11.  14.  16.  18.  20.  21.\n",
            "  22.  23.  26.  29.  33.  34.  36.  37.  39.  41.  42.  43.  44.  45.\n",
            "  52.  54.  57.  58.  59.  60.  62.  63.  66.  67.  68.  69.  71.  72.\n",
            "  74.  77.  79.  80.  85.  86.  87.  88.  89.  90.  92.  93.  94.  95.\n",
            "  96. 101. 103. 105. 107. 112. 117. 118. 120. 121. 122. 123. 124. 125.\n",
            " 127. 130. 134. 136. 137. 138. 140. 141. 142. 143. 145. 146. 150. 151.\n",
            " 152. 156. 157. 160. 162. 163. 165. 167. 169. 174. 176. 178. 179. 181.\n",
            " 182. 184. 186. 187. 188. 189. 190. 191. 196. 197. 198. 200. 202. 204.\n",
            " 205. 206. 207. 208. 210. 214. 217. 219. 222. 223. 225. 226. 228. 229.\n",
            " 230. 231. 232. 233. 235. 236. 237. 239. 242. 243. 244. 250. 252.]\n",
            "\n",
            "Datos de Entrenamiento: 2100   Datos de Testeo: 210\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 127
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 8**"
      ],
      "metadata": {
        "id": "g0V46ea2EOA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Utilizando el archivo Iris.csv que contiene información referida a la longitud y al ancho de sépalos y pétalos de tres especies de flores: iris setosa, iris versicolor e iris virginica.*\n",
        "\n",
        "*a) Entrenar una multiperceptrón que aprenda a clasificar las 3 clases de flores.*\n"
      ],
      "metadata": {
        "id": "2I8EE5_yESsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import chardet # para detectar la codificacion de caracteres usada\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import preprocessing, model_selection\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "EBeY46VZLpiM"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "\n",
        "with open('Iris.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "\n",
        "encoding = result['encoding']\n",
        "print(f\"Codificación detectada: {encoding}\")\n",
        "\n",
        "df_train = pd.read_csv('Iris.csv', encoding=encoding)\n",
        "print(\"df-train: \")\n",
        "print(df_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AFnaOzjMdco",
        "outputId": "eaf1bbf7-946e-4767-cfa2-b4f947276ea3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Codificación detectada: ascii\n",
            "df-train: \n",
            "     sepallength  sepalwidth  petallength  petalwidth           class\n",
            "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
            "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
            "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
            "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
            "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
            "..           ...         ...          ...         ...             ...\n",
            "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
            "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
            "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
            "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
            "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# valores de entradas\n",
        "X = df_train.iloc[:, :-1] # entradas --> todas las colum menos la ult (clase)\n",
        "X = np.array(X) # los pone en un array\n",
        "#print(X)\n",
        "\n",
        "# rtas esperadas\n",
        "Y = np.array(df_train['class']) # salida --> columna de clases\n",
        "#print(Y)\n",
        "\n",
        "# BINARIZO LAS SALIDAS --> devuelve 1 unicamente para la clase correspondiente\n",
        "binarizer = preprocessing.LabelBinarizer()\n",
        "\n",
        "Y_bin = binarizer.fit_transform(Y)\n",
        "print(\"Clases del dataset: \", binarizer.classes_)\n",
        "print('\\nDatos de Entrenamiento: %d   Datos de Testeo: %d' % (len(Y_bin), len(Y_test) ))\n",
        "\n",
        "# escala valores entre 0 y 1\n",
        "scaler = preprocessing.StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "#print(X)\n",
        "\n",
        "# otra forma de hacer lo anterior:\n",
        "# normalizo entrada\n",
        "#normalizarEntrada = 1\n",
        "#if normalizarEntrada:\n",
        "  # Escala valores entre 0 y 1\n",
        "  #min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  #X = min_max_scaler.fit_transform(X)\n",
        "  #X_test = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "\n",
        "df['Clase'].hist(bins = 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "1khztCuqMnNi",
        "outputId": "5a22fc25-5631-4bc7-e716-77c23b074ca0"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases del dataset:  ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
            "\n",
            "Datos de Entrenamiento: 150   Datos de Testeo: 30\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 207
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "S9Jo1nP6SFbV"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENTRENO RED\n",
        "entradas = X.shape[1] # num de columnas del arreglo\n",
        "#print (entradas)\n",
        "ocultas = 2\n",
        "salidas = 3\n",
        "print(\"SALIDASSSSSSSSSS: \", salidas)\n",
        "FunH = 'tanh'\n",
        "\n",
        "print(X.shape)      # imprime (150,4) --> (f,c) del df\n",
        "print(X_test.shape) # imrpime (30,4)\n",
        "\n",
        "\n",
        "alfa = 0.01\n",
        "EPOCAS = 500\n",
        "TAM_LOTE = 32\n",
        "\n",
        "# Creación de modelo\n",
        "modelo = Sequential()\n",
        "\n",
        "# Agrego capas\n",
        "modelo.add(Dense(ocultas, activation=FunH, input_dim=entradas)) # --> capa oculta\n",
        "modelo.add(Dense(salidas, activation = 'softmax')) # --> capa de salida\n",
        "\n",
        "modelo.summary()\n",
        "\n",
        "optimizador = optimizers.SGD(learning_rate = alfa)\n",
        "modelo.compile(optimizer=optimizador, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Convierte las etiquetas en one-hot\n",
        "Y = pd.get_dummies(Y)\n",
        "\n",
        "# Separa ejemplos para entrenamiento y testeo\n",
        "TEST_SIZE = 0.2\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=TEST_SIZE)#, random_state=42)\n",
        "print('\\nDatos de Entrenamiento: %d   Datos de Testeo: %d' % (len(Y_bin), len(Y_test) ))\n",
        "\n",
        "\n",
        "\n",
        "# Entrenamiento del modelo, guarda la historia del progreso\n",
        "modelo.fit(x=X_train, y=Y, batch_size=TAM_LOTE, epochs=EPOCAS)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "9Jm-9xNcQnO2",
        "outputId": "8988c379-face-4f4a-bc41-eafadf20dde1"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SALIDASSSSSSSSSS:  3\n",
            "(150, 4)\n",
            "(30, 4)\n",
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_74 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 3)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19 (76.00 Byte)\n",
            "Trainable params: 19 (76.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "\n",
            "Datos de Entrenamiento: 150   Datos de Testeo: 30\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-cba45fd8c22a>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Entrenamiento del modelo, guarda la historia del progreso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmodelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTAM_LOTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCAS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 120\n  y sizes: 150\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.out_activation_ = 'softmax'\n",
        "\n",
        "# Medición del entrenamiento\n",
        "Y_pred = modelo.predict(X_test)\n",
        "score = modelo.score(X_test, Y_test)\n",
        "\n",
        "# imprimo rtas\n",
        "#print(Y_pred)\n",
        "#print(Y_test)\n",
        "\n",
        "\n",
        "# para calcular efect, score, matriz confusión y no obtener formato binario\n",
        "# hacemos la transformación inversa\n",
        "#Y_pred_it = binarizer.inverse_transform(Y_pred)\n",
        "Y_test_it = binarizer.inverse_transform(Y_test_bin)\n",
        "\n",
        "# calculo manual del accuracy\n",
        "print('Efectividad: %6.2f%%' % (100*(Y_pred == Y_test_it).sum()/len(Y_test_it)) )\n",
        "print('      Score: %6.2f%%' % (score) )\n",
        "\n",
        "\n",
        "# Y_pred == Y_test --> devuelve true si la predic es correcta\n",
        "# .sum --> suma el total de true de la operación anterior\n"
      ],
      "metadata": {
        "id": "GM8CSXwMT1Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*b) Utilice Python para calcular la matriz de confusión y calcule de forma manual las métricas de precision, recall, accuracy y f1-score. Luego utilice la función classification_report de SciKit-Learn para\n",
        "comparar los resultados.*"
      ],
      "metadata": {
        "id": "Qxsjc90eQV9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calcula las metricas precision, recall, f1-score y accuracy a partir de la matriz de confusion\n",
        "# retorna tupla: ( precision, recall, f1_score, accuracy )\n",
        "def calcular_metricas(conf_mat):\n",
        "    precision = np.zeros(conf_mat.shape[0])\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        precision[i] = conf_mat[i][i]/sum(conf_mat.T[i])\n",
        "\n",
        "    recall = np.zeros(conf_mat.shape[0])\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        recall[i] = conf_mat[i][i]/sum(conf_mat[i])\n",
        "\n",
        "    f1_score = 2* (precision*recall) /(precision+recall)\n",
        "\n",
        "    accuracy =  0\n",
        "    for i in range(0, len(conf_mat)):\n",
        "        accuracy+=conf_mat[i][i]\n",
        "    accuracy/= conf_mat.sum()\n",
        "\n",
        "    return ( precision, recall, f1_score, accuracy )\n",
        "\n",
        "# el parámetro metricas es una tupla ( precision, recall, f1_score, accuracy )\n",
        "def imprimir_metricas( metricas ):\n",
        "    (precision, recall, f1_score, accuracy) = metricas\n",
        "    print('\\n clase   precision    recall    f1-score')\n",
        "    for i in range(0, len(precision)):\n",
        "        print('%5d %10.2f %10.2f %10.2f' % (i, precision[i], recall[i], f1_score[i]))\n",
        "    print('\\naccuracy: %6.2f\\n' % accuracy)\n"
      ],
      "metadata": {
        "id": "wyhltY9VQXo_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}